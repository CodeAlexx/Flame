# Autograd Remaining Operations Fix Agent

**Context**: Deadlock issue is SOLVED! The pattern is proven: use GpuOps directly in compute_gradients to avoid re-acquiring autograd lock.

**Mission**: Fix all remaining operations (matmul, conv2d, etc.) to use low-level GpuOps during backward pass, following the successful pattern.

## Agent: Operation Backward Pass Fixer

### 1. Apply the Proven Pattern to All Operations

#### Update MatMul Backward
```rust
// In compute_gradients function
Op::MatMul { lhs, rhs } => {
    let output_grad = grads.get(&op.output_id).unwrap();
    
    // OLD - CAUSES DEADLOCK:
    // let grad_lhs = output_grad.matmul(&rhs.transpose(-1, -2)?)?;
    
    // NEW - USE GPU_OPS DIRECTLY:
    let rhs_t = gpu_ops::transpose(rhs, -1, -2)?;
    let grad_lhs = gpu_ops::matmul(output_grad, &rhs_t)?;
    
    let lhs_t = gpu_ops::transpose(lhs, -1, -2)?;  
    let grad_rhs = gpu_ops::matmul(&lhs_t, output_grad)?;
    
    vec![
        (lhs.id(), grad_lhs),
        (rhs.id(), grad_rhs),
    ]
}
```

#### Update Conv2D Backward
```rust
Op::Conv2d { input, weight, bias, stride, padding, .. } => {
    let output_grad = grads.get(&op.output_id).unwrap();
    
    // Use direct GPU operations, not tensor methods
    let grad_input = gpu_ops::conv2d_backward_input(
        output_grad,
        weight,
        input.shape(),
        *stride,
        *padding
    )?;
    
    let grad_weight = gpu_ops::conv2d_backward_weight(
        input,
        output_grad,
        weight.shape(),
        *stride,
        *padding
    )?;
    
    let mut grads = vec![
        (input.id(), grad_input),
        (weight.id(), grad_weight),
    ];
    
    if let Some(bias) = bias {
        // Use gpu_ops for bias gradient too
        let grad_bias = gpu_ops::sum_dims(output_grad, &[0, 2, 3])?;
        grads.push((bias.id(), grad_bias));
    }
    
    grads
}
```

#### Update ReLU Backward
```rust
Op::ReLU { input } => {
    let output_grad = grads.get(&op.output_id).unwrap();
    
    // ReLU gradient: output_grad where input > 0, else 0
    let grad_input = gpu_ops::relu_backward(output_grad, input)?;
    
    vec![(input.id(), grad_input)]
}
```

#### Update Sum/Mean Backward
```rust
Op::Sum { input, dims } => {
    let output_grad = grads.get(&op.output_id).unwrap();
    
    // Broadcast gradient back to input shape
    let grad_input = gpu_ops::broadcast_to(output_grad, input.shape())?;
    
    vec![(input.id(), grad_input)]
}

Op::Mean { input, dims } => {
    let output_grad = grads.get(&op.output_id).unwrap();
    
    // Scale by number of elements that were averaged
    let num_elements = dims.iter().map(|&d| input.shape()[d]).product::<usize>() as f32;
    let scaled_grad = gpu_ops::div_scalar(output_grad, num_elements)?;
    let grad_input = gpu_ops::broadcast_to(&scaled_grad, input.shape())?;
    
    vec![(input.id(), grad_input)]
}
```

#### Update Reshape/View Backward
```rust
Op::Reshape { input, old_shape } => {
    let output_grad = grads.get(&op.output_id).unwrap();
    
    // Just reshape gradient back to original shape
    let grad_input = gpu_ops::reshape(output_grad, old_shape)?;
    
    vec![(input.id(), grad_input)]
}
```

### 2. Ensure All GPU Operations Exist

#### Check What's Available in gpu_ops
```rust
// Verify these functions exist in gpu_ops module:
mod gpu_ops {
    pub fn matmul(a: &Tensor, b: &Tensor) -> Result<Tensor>;
    pub fn transpose(input: &Tensor, dim1: i32, dim2: i32) -> Result<Tensor>;
    pub fn conv2d_backward_input(...) -> Result<Tensor>;
    pub fn conv2d_backward_weight(...) -> Result<Tensor>;
    pub fn relu_backward(grad_out: &Tensor, input: &Tensor) -> Result<Tensor>;
    pub fn sum_dims(input: &Tensor, dims: &[usize]) -> Result<Tensor>;
    pub fn broadcast_to(input: &Tensor, shape: &[usize]) -> Result<Tensor>;
    pub fn div_scalar(input: &Tensor, scalar: f32) -> Result<Tensor>;
    pub fn reshape(input: &Tensor, shape: &[usize]) -> Result<Tensor>;
}
```

#### Implement Missing GPU Operations
```rust
// If any are missing, implement them as direct CUDA calls
impl gpu_ops {
    pub fn relu_backward(grad_out: &Tensor, input: &Tensor) -> Result<Tensor> {
        let mut grad_input = Tensor::zeros_like(input)?;
        
        unsafe {
            launch_kernel!(
                "relu_backward_kernel",
                input.device(),
                (input.numel(),),
                grad_out.data_ptr(),
                input.data_ptr(),
                grad_input.data_ptr(),
                input.numel()
            )?;
        }
        
        Ok(grad_input)
    }
    
    pub fn broadcast_to(input: &Tensor, target_shape: &[usize]) -> Result<Tensor> {
        let mut output = Tensor::zeros(target_shape, input.device())?;
        
        unsafe {
            launch_kernel!(
                "broadcast_kernel",
                input.device(),
                (output.numel(),),
                input.data_ptr(),
                output.data_ptr(),
                input.shape().as_ptr(),
                target_shape.as_ptr(),
                input.dims(),
                target_shape.len()
            )?;
        }
        
        Ok(output)
    }
}
```

### 3. Test Each Operation Fix

#### Test MatMul Backward
```rust
#[test]
fn test_matmul_backward_fixed() {
    let device = Device::Cuda(0);
    
    let a = Tensor::randn(&[2, 3], device)?.requires_grad();
    let b = Tensor::randn(&[3, 4], device)?.requires_grad();
    
    let c = a.matmul(&b)?;
    let loss = c.sum();
    
    // This should NOT hang anymore
    let grad_map = loss.backward()?;
    
    assert!(grad_map.contains_key(&a.id()));
    assert!(grad_map.contains_key(&b.id()));
    
    println!("✅ MatMul backward works!");
}
```

#### Test Conv2D Backward
```rust
#[test]
fn test_conv2d_backward_fixed() {
    let device = Device::Cuda(0);
    
    let input = Tensor::randn(&[1, 3, 8, 8], device)?.requires_grad();
    let conv = Conv2d::new(3, 16, 3, 1, 1, device)?;
    
    let output = conv.forward(&input)?;
    let loss = output.sum();
    
    // This should NOT hang anymore
    let grad_map = loss.backward()?;
    
    assert!(grad_map.contains_key(&input.id()));
    
    println!("✅ Conv2D backward works!");
}
```

#### Test Complete CNN Backward
```rust
#[test]
fn test_complete_cnn_backward() {
    let device = Device::Cuda(0);
    
    let input = Tensor::randn(&[2, 3, 16, 16], device)?.requires_grad();
    
    let conv1 = Conv2d::new(3, 32, 3, 1, 1, device)?;
    let conv2 = Conv2d::new(32, 64, 3, 1, 1, device)?;
    
    let h1 = conv1.forward(&input)?.relu()?;
    let h2 = conv2.forward(&h1)?.relu()?;
    let h3 = h2.mean(); // Global average pool
    let loss = h3.sum();
    
    // This is the REAL test - complex graph should work
    let grad_map = loss.backward()?;
    
    assert!(grad_map.contains_key(&input.id()));
    
    println!("✅ Complete CNN backward works!");
}
```

### 4. Test with Realistic Image/Video Operations

#### Test Diffusion Training Operations
```rust
#[test]
fn test_diffusion_operations_complete() {
    let device = Device::Cuda(0);
    
    // Realistic diffusion model operations
    let latents = Tensor::randn(&[4, 4, 32, 32], device)?.requires_grad();
    let noise = Tensor::randn(&[4, 4, 32, 32], device)?;
    let timesteps = Tensor::randint(0, 1000, &[4], device)?;
    
    // Add noise (diffusion forward)
    let noisy_latents = &latents + &noise * 0.1;
    
    // Simple U-Net style prediction
    let conv_in = Conv2d::new(4, 64, 3, 1, 1, device)?;
    let conv_mid = Conv2d::new(64, 128, 3, 1, 1, device)?;
    let conv_out = Conv2d::new(128, 4, 3, 1, 1, device)?;
    
    let h1 = conv_in.forward(&noisy_latents)?.relu()?;
    let h2 = conv_mid.forward(&h1)?.relu()?;
    let pred_noise = conv_out.forward(&h2)?;
    
    // MSE loss
    let diff = &pred_noise - &noise;
    let loss = (&diff * &diff)?.mean();
    
    // This should work for diffusion training
    let grad_map = loss.backward()?;
    
    assert!(grad_map.contains_key(&latents.id()));
    
    println!("✅ Diffusion training operations work!");
}
```

### 5. Systematic Operation Checklist

#### All Operations to Fix
- [ ] MatMul backward
- [ ] Conv2D backward
- [ ] ReLU backward  
- [ ] Sum backward
- [ ] Mean backward
- [ ] Reshape/View backward
- [ ] Transpose backward
- [ ] Pow backward
- [ ] Add/Sub/Mul/Div backward (already working)

#### For Each Operation:
1. Update compute_gradients to use gpu_ops
2. Ensure gpu_ops function exists
3. Test with simple case
4. Test with complex graph
5. Verify no deadlock

## Success Criteria

**COMPLETE when:**
- [ ] All operations use gpu_ops in backward pass
- [ ] No autograd deadlocks on any operation
- [ ] Complex CNN graphs work
- [ ] Diffusion model operations work
- [ ] Complete training loops work

**This fixes autograd completely for image/video model training.**
