//! Flash Attention implementation for Flame
//! 
//! Efficient attention mechanism that reduces memory usage from O(NÂ²) to O(N)
//! Based on "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"

use crate::{Tensor, Shape, Result, FlameError, CudaDevice, DType};
use std::sync::Arc;

/// Flash Attention configuration
pub struct FlashAttentionConfig {
    /// Dropout probability (0.0 = no dropout)
    pub dropout_p: f32,
    /// Whether to use causal mask (for autoregressive models)
    pub is_causal: bool,
    /// Sliding window size (None = full attention)
    pub window_size: Option<usize>,
    /// Softmax scale (None = 1/sqrt(head_dim))
    pub softmax_scale: Option<f32>,
}

impl Default for FlashAttentionConfig {
    fn default() -> Self {
        Self {
            dropout_p: 0.0,
            is_causal: false,
            window_size: None,
            softmax_scale: None,
        }
    }
}

/// Flash Attention implementation
pub struct FlashAttention {
    config: FlashAttentionConfig,
    device: Arc<CudaDevice>,
}

impl FlashAttention {
    pub fn new(config: FlashAttentionConfig, device: Arc<CudaDevice>) -> Self {
        Self { config, device }
    }
    
    /// Forward pass for Flash Attention
    /// 
    /// # Arguments
    /// * `q` - Query tensor [batch_size, seq_len, num_heads, head_dim]
    /// * `k` - Key tensor [batch_size, seq_len_kv, num_heads, head_dim]
    /// * `v` - Value tensor [batch_size, seq_len_kv, num_heads, head_dim]
    /// 
    /// # Returns
    /// * Output tensor [batch_size, seq_len, num_heads, head_dim]
    pub fn forward(&self, q: &Tensor, k: &Tensor, v: &Tensor) -> Result<Tensor> {
        // Validate shapes
        let q_shape = q.shape().dims();
        let k_shape = k.shape().dims();
        let v_shape = v.shape().dims();
        
        if q_shape.len() != 4 || k_shape.len() != 4 || v_shape.len() != 4 {
            return Err(FlameError::InvalidOperation(
                "Flash attention expects 4D tensors [batch, seq_len, num_heads, head_dim]".into()
            ));
        }
        
        let (batch_size, seq_len_q, num_heads, head_dim) = (
            q_shape[0], q_shape[1], q_shape[2], q_shape[3]
        );
        let seq_len_kv = k_shape[1];
        
        // Validate K and V have same sequence length
        if k_shape[1] != v_shape[1] {
            return Err(FlameError::InvalidOperation(
                format!("K and V must have same sequence length, got {} and {}", k_shape[1], v_shape[1])
            ));
        }
        
        // Validate batch size and num_heads match
        if k_shape[0] != batch_size || v_shape[0] != batch_size {
            return Err(FlameError::InvalidOperation(
                "Batch size mismatch in Q, K, V".into()
            ));
        }
        
        if k_shape[2] != num_heads || v_shape[2] != num_heads {
            return Err(FlameError::InvalidOperation(
                "Number of heads mismatch in Q, K, V".into()
            ));
        }
        
        if k_shape[3] != head_dim || v_shape[3] != head_dim {
            return Err(FlameError::InvalidOperation(
                "Head dimension mismatch in Q, K, V".into()
            ));
        }
        
        // Calculate softmax scale
        let softmax_scale = self.config.softmax_scale
            .unwrap_or(1.0 / (head_dim as f32).sqrt());
        
        // Use standard attention for now
        // TODO: Add optimized tiled/Flash implementation when F16 is stable
        self.standard_attention(q, k, v, softmax_scale)
    }
    
    /// Standard attention implementation (fallback)
    fn standard_attention(&self, q: &Tensor, k: &Tensor, v: &Tensor, scale: f32) -> Result<Tensor> {
        // Q: [batch, seq_len_q, num_heads, head_dim]
        // K: [batch, seq_len_kv, num_heads, head_dim]
        // V: [batch, seq_len_kv, num_heads, head_dim]
        
        // Transpose for matmul: [batch, num_heads, seq_len, head_dim]
        let q = q.permute(&[0, 2, 1, 3])?;
        let k = k.permute(&[0, 2, 1, 3])?;
        let v = v.permute(&[0, 2, 1, 3])?;
        
        // Scale Q
        let q_scaled = q.mul_scalar(scale)?;
        
        // Compute attention scores: Q @ K^T
        // [batch, num_heads, seq_len_q, head_dim] @ [batch, num_heads, head_dim, seq_len_kv]
        // = [batch, num_heads, seq_len_q, seq_len_kv]
        let k_transposed = k.transpose_dims(2, 3)?;
        let scores = q_scaled.bmm(&k_transposed)?;
        
        // Apply causal mask if needed
        let scores = if self.config.is_causal {
            self.apply_causal_mask(&scores)?
        } else {
            scores
        };
        
        // Softmax
        let attn_weights = scores.softmax(-1)?;
        
        // Apply dropout if needed (skip for now)
        let attn_weights = if self.config.dropout_p > 0.0 {
            // TODO: Implement dropout when needed
            attn_weights
        } else {
            attn_weights
        };
        
        // Compute output: attn_weights @ V
        // [batch, num_heads, seq_len_q, seq_len_kv] @ [batch, num_heads, seq_len_kv, head_dim]
        // = [batch, num_heads, seq_len_q, head_dim]
        let output = attn_weights.bmm(&v)?;
        
        // Transpose back: [batch, seq_len_q, num_heads, head_dim]
        output.permute(&[0, 2, 1, 3])
    }
    
    /// Apply causal mask to attention scores
    fn apply_causal_mask(&self, scores: &Tensor) -> Result<Tensor> {
        let dims = scores.shape().dims();
        let seq_len_q = dims[2];
        let seq_len_kv = dims[3];
        
        // Create causal mask
        let mut mask_data = vec![-1e9f32; seq_len_q * seq_len_kv];
        for i in 0..seq_len_q {
            for j in 0..=i.min(seq_len_kv - 1) {
                mask_data[i * seq_len_kv + j] = 0.0;
            }
        }
        
        // Create mask tensor
        let mask = Tensor::from_vec(
            mask_data,
            Shape::from_dims(&[1, 1, seq_len_q, seq_len_kv]),
            self.device.clone()
        )?;
        
        // Add mask to scores (broadcasting will handle batch and heads dimensions)
        scores.add(&mask)
    }
}

/// Simple API for Flash Attention
///
/// This implements scaled dot-product attention, `softmax(Q @ K^T . softmax_scale) @ V`.
/// Multi-query and grouped-query attention are supported by using tensors k and v with fewer heads
/// than q, the number of heads in k and v has to be divisible by the number of heads in q.
///
/// # Arguments
///
/// * `q` - Query tensor with shape `(batch, seq_len_q, num_heads, head_size)`.
/// * `k` - Key tensor with shape `(batch, seq_len_kv, num_heads_kv, head_size)`.
/// * `v` - Value tensor with shape `(batch, seq_len_kv, num_heads_kv, head_size)`.
/// * `softmax_scale` - Scale factor for the softmax.
/// * `causal` - Whether to apply causal masking.
///
/// The resulting tensor has dimensions `(batch, seq_len_q, num_heads, head_size)`.
pub fn flash_attn(
    q: &Tensor,
    k: &Tensor,
    v: &Tensor,
    softmax_scale: f32,
    causal: bool,
) -> Result<Tensor> {
    let device = q.device();
    let config = FlashAttentionConfig {
        is_causal: causal,
        softmax_scale: Some(softmax_scale),
        ..Default::default()
    };
    let flash_attn = FlashAttention::new(config, device.clone());
    flash_attn.forward(q, k, v)
}

/// Flash Attention with variable-length sequences.
///
/// # Arguments
///
/// * `q` - Query tensor with shape `(total_q, num_heads, head_size)`.
/// * `k` - Key tensor with shape `(total_kv, num_heads_kv, head_size)`.
/// * `v` - Value tensor with shape `(total_kv, num_heads_kv, head_size)`.
/// * `seqlens_q` - Cumulative sequence lengths for queries.
/// * `seqlens_k` - Cumulative sequence lengths for keys/values.
/// * `max_seqlen_q` - Maximum sequence length in queries.
/// * `max_seqlen_k` - Maximum sequence length in keys/values.
/// * `softmax_scale` - Scale factor for the softmax.
/// * `causal` - Whether to apply causal masking.
///
/// The resulting tensor has dimensions `(total_q, num_heads, head_size)`.
pub fn flash_attn_varlen(
    q: &Tensor,
    k: &Tensor,
    v: &Tensor,
    seqlens_q: &Tensor,
    seqlens_k: &Tensor,
    max_seqlen_q: usize,
    max_seqlen_k: usize,
    softmax_scale: f32,
    causal: bool,
) -> Result<Tensor> {
    // For now, this falls back to the regular implementation
    // TODO: Implement variable-length support with cu_seqlens
    flash_attn(q, k, v, softmax_scale, causal)
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_flash_attention() -> Result<()> {
        let device = Arc::new(CudaDevice::new(0)?);
        
        let config = FlashAttentionConfig::default();
        let flash_attn = FlashAttention::new(config, device.clone());
        
        // Test tensors
        let batch_size = 2;
        let seq_len = 8;
        let num_heads = 4;
        let head_dim = 16;
        
        let q = Tensor::randn(
            Shape::from_dims(&[batch_size, seq_len, num_heads, head_dim]),
            0.0, 0.1, device.clone()
        )?;
        
        let k = Tensor::randn(
            Shape::from_dims(&[batch_size, seq_len, num_heads, head_dim]),
            0.0, 0.1, device.clone()
        )?;
        
        let v = Tensor::randn(
            Shape::from_dims(&[batch_size, seq_len, num_heads, head_dim]),
            0.0, 0.1, device.clone()
        )?;
        
        let output = flash_attn.forward(&q, &k, &v)?;
        
        // Check output shape
        assert_eq!(output.shape().dims(), &[batch_size, seq_len, num_heads, head_dim]);
        
        println!("Flash Attention test passed!");
        Ok(())
    }
    
    #[test]
    fn test_causal_flash_attention() -> Result<()> {
        let device = Arc::new(CudaDevice::new(0)?);
        
        let config = FlashAttentionConfig {
            is_causal: true,
            ..Default::default()
        };
        let flash_attn = FlashAttention::new(config, device.clone());
        
        // Test with causal mask
        let batch_size = 1;
        let seq_len = 4;
        let num_heads = 2;
        let head_dim = 8;
        
        let q = Tensor::randn(
            Shape::from_dims(&[batch_size, seq_len, num_heads, head_dim]),
            0.0, 0.1, device.clone()
        )?;
        
        let k = Tensor::randn(
            Shape::from_dims(&[batch_size, seq_len, num_heads, head_dim]),
            0.0, 0.1, device.clone()
        )?;
        
        let v = Tensor::ones(
            Shape::from_dims(&[batch_size, seq_len, num_heads, head_dim]),
            device.clone()
        )?;
        
        let output = flash_attn.forward(&q, &k, &v)?;
        
        // Check output shape
        assert_eq!(output.shape().dims(), &[batch_size, seq_len, num_heads, head_dim]);
        
        println!("Causal Flash Attention test passed!");
        Ok(())
    }
}