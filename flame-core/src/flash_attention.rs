//! Flash Attention implementation for Flame
//! 
//! Efficient attention mechanism that reduces memory usage from O(NÂ²) to O(N)
//! Based on "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"

use crate::{Tensor, Shape, Result, FlameError, CudaDevice, DType};
use std::sync::Arc;
use cudarc::driver::LaunchAsync;

/// Flash Attention configuration
pub struct FlashAttentionConfig {
    /// Dropout probability (0.0 = no dropout)
    pub dropout_p: f32,
    /// Whether to use causal mask (for autoregressive models)
    pub is_causal: bool,
    /// Sliding window size (None = full attention)
    pub window_size: Option<usize>,
    /// Softmax scale (None = 1/sqrt(head_dim))
    pub softmax_scale: Option<f32>,
}

impl Default for FlashAttentionConfig {
    fn default() -> Self {
        Self {
            dropout_p: 0.0,
            is_causal: false,
            window_size: None,
            softmax_scale: None,
        }
    }
}

/// Flash Attention implementation
pub struct FlashAttention {
    config: FlashAttentionConfig,
    device: Arc<CudaDevice>,
}

impl FlashAttention {
    pub fn new(config: FlashAttentionConfig, device: Arc<CudaDevice>) -> Self {
        Self { config, device }
    }
    
    /// Forward pass for Flash Attention
    /// 
    /// # Arguments
    /// * `q` - Query tensor [batch_size, seq_len, num_heads, head_dim]
    /// * `k` - Key tensor [batch_size, seq_len_kv, num_heads, head_dim]
    /// * `v` - Value tensor [batch_size, seq_len_kv, num_heads, head_dim]
    /// 
    /// # Returns
    /// * Output tensor [batch_size, seq_len, num_heads, head_dim]
    pub fn forward(&self, q: &Tensor, k: &Tensor, v: &Tensor) -> Result<Tensor> {
        // Validate shapes
        let q_shape = q.shape().dims();
        let k_shape = k.shape().dims();
        let v_shape = v.shape().dims();
        
        if q_shape.len() != 4 || k_shape.len() != 4 || v_shape.len() != 4 {
            return Err(FlameError::InvalidOperation(
                "Flash attention expects 4D tensors [batch, seq_len, num_heads, head_dim]".into()
            ));
        }
        
        let (batch_size, seq_len_q, num_heads, head_dim) = (
            q_shape[0], q_shape[1], q_shape[2], q_shape[3]
        );
        let seq_len_kv = k_shape[1];
        
        // Validate K and V have same sequence length
        if k_shape[1] != v_shape[1] {
            return Err(FlameError::InvalidOperation(
                format!("K and V must have same sequence length, got {} and {}", k_shape[1], v_shape[1])
            ));
        }
        
        // Validate batch size and num_heads match
        if k_shape[0] != batch_size || v_shape[0] != batch_size {
            return Err(FlameError::InvalidOperation(
                "Batch size mismatch in Q, K, V".into()
            ));
        }
        
        if k_shape[2] != num_heads || v_shape[2] != num_heads {
            return Err(FlameError::InvalidOperation(
                "Number of heads mismatch in Q, K, V".into()
            ));
        }
        
        if k_shape[3] != head_dim || v_shape[3] != head_dim {
            return Err(FlameError::InvalidOperation(
                "Head dimension mismatch in Q, K, V".into()
            ));
        }
        
        // Calculate softmax scale
        let softmax_scale = self.config.softmax_scale
            .unwrap_or(1.0 / (head_dim as f32).sqrt());
        
        // Use Flash Attention if conditions are met
        // Note: Flash attention kernels require F16/BF16, so we need to check dtype
        if self.can_use_flash_attention(seq_len_q, seq_len_kv, head_dim) {
            // For now, always use the tiled CPU implementation
            // TODO: Add F16/BF16 support for GPU kernels
            self.flash_attention_forward(q, k, v, softmax_scale)
        } else {
            // Fall back to standard attention for unsupported configurations
            self.standard_attention(q, k, v, softmax_scale)
        }
    }
    
    /// Check if we can use Flash Attention for given dimensions
    fn can_use_flash_attention(&self, seq_len_q: usize, seq_len_kv: usize, head_dim: usize) -> bool {
        // Flash Attention works best with certain constraints
        // Check if dimensions are suitable
        head_dim % 8 == 0 && // Head dimension should be multiple of 8
        head_dim <= 256 &&   // Max head dimension
        seq_len_q <= 16384 && // Max sequence length
        seq_len_kv <= 16384
    }
    
    /// Flash Attention using CUDA kernels
    fn flash_attention_cuda_kernel(&self, q: &Tensor, k: &Tensor, v: &Tensor, scale: f32) -> Result<Tensor> {
        // This implementation uses the optimized CUDA kernels from flash-attention
        let q_shape = q.shape().dims();
        let (batch_size, seq_len_q, num_heads, head_dim) = (
            q_shape[0], q_shape[1], q_shape[2], q_shape[3]
        );
        let seq_len_kv = k.shape().dims()[1];
        let num_heads_k = k.shape().dims()[2];
        
        // Prepare window sizes
        let mut window_size_left = self.config.window_size_left
            .filter(|v| v <= &seq_len_kv)
            .map(|v| v as i32)
            .unwrap_or(-1);
        let mut window_size_right = self.config.window_size_right
            .filter(|v| v <= &seq_len_kv)
            .map(|v| v as i32)
            .unwrap_or(-1);
        
        // Special handling for causal
        let is_causal = if self.config.is_causal {
            window_size_right = 0;
            1
        } else {
            0
        };
        
        // Adjust window sizes for local attention
        if window_size_left < 0 && window_size_right >= 0 {
            window_size_left = seq_len_kv as i32;
        }
        if window_size_left >= 0 && window_size_right < 0 {
            window_size_right = seq_len_kv as i32;
        }
        
        // Round dimensions for kernel efficiency
        let head_size = round_multiple(head_dim, 8);
        let head_size_rounded = round_multiple(head_size, 32);
        let seqlen_q_rounded = round_multiple(seq_len_q, 128);
        let seqlen_k_rounded = round_multiple(seq_len_kv, 128);
        
        // Allocate output and workspace
        let out_shape = q.shape().clone();
        let output = unsafe { self.device.alloc::<f16>(out_shape.elem_count())? };
        let softmax_lse = self.device.alloc_zeros::<f32>(batch_size * 128 * num_heads * seq_len_q)?;
        
        // Handle alibi slopes if provided
        let alibi_slopes_ptr = if let Some(alibi_slopes) = &self.config.alibi_slopes {
            alibi_slopes.data()
        } else {
            std::ptr::null()
        };
        
        // Get tensor strides
        let q_strides = q.stride();
        let k_strides = k.stride();
        let v_strides = v.stride();
        let o_strides = vec![seq_len_q * num_heads * head_dim, num_heads * head_dim, head_dim, 1];
        
        let is_bf16 = match q.dtype() {
            DType::BF16 => 1,
            DType::F16 => 0,
            _ => return Err(FlameError::InvalidOperation("Flash attention only supports F16/BF16".into())),
        };
        
        // Call the CUDA kernel
        unsafe {
            ffi::run_mha(
                q.data() as *const c_void,
                k.data() as *const c_void,
                v.data() as *const c_void,
                output.as_ptr() as *const c_void,
                softmax_lse.as_ptr() as *const c_void,
                alibi_slopes_ptr as *const c_void,
                std::ptr::null(), // cu_seqlens_q_ptr
                std::ptr::null(), // cu_seqlens_k_ptr
                q_strides[0] as u32, // q_batch_stride
                k_strides[0] as u32, // k_batch_stride
                v_strides[0] as u32, // v_batch_stride
                o_strides[0] as u32, // o_batch_stride
                0, // alibi_slopes_batch_stride
                q_strides[1] as u32, // q_row_stride
                k_strides[1] as u32, // k_row_stride
                v_strides[1] as u32, // v_row_stride
                o_strides[1] as u32, // o_row_stride
                q_strides[2] as u32, // q_head_stride
                k_strides[2] as u32, // k_head_stride
                v_strides[2] as u32, // v_head_stride
                o_strides[2] as u32, // o_head_stride
                batch_size as u32,
                num_heads as u32,
                num_heads_k as u32,
                head_size as u32,
                head_size_rounded as u32,
                scale,
                seq_len_q as u32,
                seq_len_kv as u32,
                seqlen_q_rounded as u32,
                seqlen_k_rounded as u32,
                is_bf16,
                is_causal,
                0, // unpadded_lse
                window_size_left,
                window_size_right,
                self.config.softcap.unwrap_or(0.0),
            );
        }
        
        // Wrap output in a Tensor
        Ok(crate::cuda_kernels::create_output_tensor(
            output,
            out_shape,
            self.device.clone()
        )?)
    }
    
    /// Flash Attention forward with tiling (CPU implementation)
    fn flash_attention_forward(&self, q: &Tensor, k: &Tensor, v: &Tensor, scale: f32) -> Result<Tensor> {
        // Implement the tiled Flash Attention algorithm
        // This is a simplified version - production would use optimized CUDA kernels
        
        let q_shape = q.shape().dims();
        let (batch_size, seq_len_q, num_heads, head_dim) = (
            q_shape[0], q_shape[1], q_shape[2], q_shape[3]
        );
        let seq_len_kv = k.shape().dims()[1];
        
        // Tile sizes for Flash Attention
        let br = 32.min(seq_len_q); // Block size for queries
        let bc = 32.min(seq_len_kv); // Block size for keys/values
        
        // Initialize output
        let mut output = Tensor::zeros(q.shape().clone(), q.device().clone())?;
        
        // Process in tiles
        for batch_idx in 0..batch_size {
            for head_idx in 0..num_heads {
                // Process this batch and head
                for q_start in (0..seq_len_q).step_by(br) {
                    let q_end = (q_start + br).min(seq_len_q);
                    
                    // Initialize row statistics
                    let mut row_max = vec![-f32::INFINITY; q_end - q_start];
                    let mut row_sum = vec![0.0f32; q_end - q_start];
                    
                    // Extract Q block for this tile
                    let q_block = self.extract_block(q, batch_idx, head_idx, q_start, q_end, 0, head_dim)?;
                    
                    // Initialize output block
                    let mut out_block = vec![0.0f32; (q_end - q_start) * head_dim];
                    
                    // Process K,V in tiles
                    for kv_start in (0..seq_len_kv).step_by(bc) {
                        let kv_end = (kv_start + bc).min(seq_len_kv);
                        
                        // Check causal mask
                        if self.config.is_causal && kv_start > q_end {
                            break; // Skip future tokens
                        }
                        
                        // Extract K,V blocks
                        let k_block = self.extract_block(k, batch_idx, head_idx, kv_start, kv_end, 0, head_dim)?;
                        let v_block = self.extract_block(v, batch_idx, head_idx, kv_start, kv_end, 0, head_dim)?;
                        
                        // Compute QK^T for this block
                        let scores = self.compute_block_scores(&q_block, &k_block, scale, 
                                                              q_end - q_start, kv_end - kv_start, head_dim)?;
                        
                        // Apply causal mask if needed
                        let scores = if self.config.is_causal {
                            self.apply_causal_mask_block(scores, q_start, kv_start, 
                                                        q_end - q_start, kv_end - kv_start)?
                        } else {
                            scores
                        };
                        
                        // Update statistics and compute softmax incrementally
                        self.update_statistics_and_output(&scores, &v_block, 
                                                         &mut row_max, &mut row_sum, &mut out_block,
                                                         q_end - q_start, kv_end - kv_start, head_dim)?;
                    }
                    
                    // Write output block back
                    self.write_block(&mut output, &out_block, batch_idx, head_idx, 
                                    q_start, q_end, 0, head_dim)?;
                }
            }
        }
        
        Ok(output)
    }
    
    /// Extract a block from a 4D tensor
    fn extract_block(&self, tensor: &Tensor, batch: usize, head: usize, 
                     seq_start: usize, seq_end: usize, 
                     feat_start: usize, feat_end: usize) -> Result<Vec<f32>> {
        let data = tensor.to_vec()?;
        let dims = tensor.shape().dims();
        let seq_len = dims[1];
        let num_heads = dims[2];
        let head_dim = dims[3];
        
        let mut block = Vec::new();
        for seq_idx in seq_start..seq_end {
            for feat_idx in feat_start..feat_end {
                let idx = batch * seq_len * num_heads * head_dim +
                         seq_idx * num_heads * head_dim +
                         head * head_dim +
                         feat_idx;
                block.push(data[idx]);
            }
        }
        
        Ok(block)
    }
    
    /// Compute scores for a block of Q and K
    fn compute_block_scores(&self, q_block: &[f32], k_block: &[f32], scale: f32,
                           q_size: usize, k_size: usize, head_dim: usize) -> Result<Vec<f32>> {
        let mut scores = vec![0.0f32; q_size * k_size];
        
        // Compute Q @ K^T
        for i in 0..q_size {
            for j in 0..k_size {
                let mut sum = 0.0;
                for d in 0..head_dim {
                    sum += q_block[i * head_dim + d] * k_block[j * head_dim + d];
                }
                scores[i * k_size + j] = sum * scale;
            }
        }
        
        Ok(scores)
    }
    
    /// Apply causal mask to a block of scores
    fn apply_causal_mask_block(&self, mut scores: Vec<f32>, q_start: usize, kv_start: usize,
                               q_size: usize, kv_size: usize) -> Result<Vec<f32>> {
        for i in 0..q_size {
            for j in 0..kv_size {
                if q_start + i < kv_start + j {
                    scores[i * kv_size + j] = -1e9;
                }
            }
        }
        Ok(scores)
    }
    
    /// Update statistics and output incrementally (Flash Attention algorithm)
    fn update_statistics_and_output(&self, scores: &[f32], v_block: &[f32],
                                   row_max: &mut [f32], row_sum: &mut [f32], 
                                   output: &mut [f32],
                                   q_size: usize, kv_size: usize, head_dim: usize) -> Result<()> {
        for i in 0..q_size {
            // Find new max for this row
            let mut new_max = row_max[i];
            for j in 0..kv_size {
                new_max = new_max.max(scores[i * kv_size + j]);
            }
            
            // Compute correction factor
            let correction = (row_max[i] - new_max).exp();
            
            // Update sum
            row_sum[i] *= correction;
            
            // Add contributions from this block
            let mut block_sum = 0.0;
            for j in 0..kv_size {
                let exp_score = (scores[i * kv_size + j] - new_max).exp();
                block_sum += exp_score;
                
                // Update output with this attention weight
                let attn_weight = exp_score;
                for d in 0..head_dim {
                    output[i * head_dim + d] = output[i * head_dim + d] * correction + 
                                              attn_weight * v_block[j * head_dim + d];
                }
            }
            
            // Update statistics
            row_sum[i] += block_sum;
            row_max[i] = new_max;
        }
        
        // Normalize by sum
        for i in 0..q_size {
            if row_sum[i] > 0.0 {
                for d in 0..head_dim {
                    output[i * head_dim + d] /= row_sum[i];
                }
            }
        }
        
        Ok(())
    }
    
    /// Write a block back to the output tensor
    fn write_block(&self, output: &mut Tensor, block: &[f32], 
                   batch: usize, head: usize,
                   seq_start: usize, seq_end: usize,
                   feat_start: usize, feat_end: usize) -> Result<()> {
        // This is a simplified version - in production we'd use GPU slice operations
        let mut data = output.to_vec()?;
        let dims = output.shape().dims();
        let seq_len = dims[1];
        let num_heads = dims[2];
        let head_dim = dims[3];
        
        let mut block_idx = 0;
        for seq_idx in seq_start..seq_end {
            for feat_idx in feat_start..feat_end {
                let idx = batch * seq_len * num_heads * head_dim +
                         seq_idx * num_heads * head_dim +
                         head * head_dim +
                         feat_idx;
                data[idx] = block[block_idx];
                block_idx += 1;
            }
        }
        
        *output = Tensor::from_vec(data, output.shape().clone(), output.device().clone())?;
        Ok(())
    }
    
    /// Standard attention implementation (fallback)
    fn standard_attention(&self, q: &Tensor, k: &Tensor, v: &Tensor, scale: f32) -> Result<Tensor> {
        // Q: [batch, seq_len_q, num_heads, head_dim]
        // K: [batch, seq_len_kv, num_heads, head_dim]
        // V: [batch, seq_len_kv, num_heads, head_dim]
        
        // Transpose for matmul: [batch, num_heads, seq_len, head_dim]
        let q = q.permute(&[0, 2, 1, 3])?;
        let k = k.permute(&[0, 2, 1, 3])?;
        let v = v.permute(&[0, 2, 1, 3])?;
        
        // Scale Q
        let q_scaled = q.mul_scalar(scale)?;
        
        // Compute attention scores: Q @ K^T
        // [batch, num_heads, seq_len_q, head_dim] @ [batch, num_heads, head_dim, seq_len_kv]
        // = [batch, num_heads, seq_len_q, seq_len_kv]
        let k_transposed = k.transpose_dims(2, 3)?;
        let scores = q_scaled.bmm(&k_transposed)?;
        
        // Apply causal mask if needed
        let scores = if self.config.is_causal {
            self.apply_causal_mask(&scores)?
        } else {
            scores
        };
        
        // Softmax
        let attn_weights = self.softmax(&scores, -1)?;
        
        // Apply dropout if needed
        let attn_weights = if self.config.dropout_p > 0.0 {
            // Now actually tries to apply dropout
            let kernel_code = r#"
extern "C" __global__ void dropout_kernel(
    float *output,
    const float *input,
    const float *random_vals,
    float dropout_p,
    float scale,
    int numel
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < numel) {
        if (random_vals[idx] < dropout_p) {
            output[idx] = 0.0f;
        } else {
            output[idx] = input[idx] * scale;
        }
    }
}"#;
            
            // Generate random values for dropout mask
            let numel = attn_weights.shape().elem_count();
            let mut rng_data = vec![0.0f32; numel];
            use rand::Rng;
            let mut rng = rand::thread_rng();
            for i in 0..numel {
                rng_data[i] = rng.gen::<f32>();
            }
            
            let random_vals = self.device.htod_sync_copy(&rng_data)
                .map_err(|_| FlameError::CudaDriver)?;
            
            crate::cuda_kernels::CudaKernels::ensure_kernel(&self.device, "dropout_kernel", kernel_code)?;
            
            let f = self.device.get_func("dropout_kernel", "dropout_kernel")
                .ok_or_else(|| FlameError::Cuda("Failed to get dropout kernel".into()))?;
            
            let mut output_data = unsafe { self.device.alloc::<f32>(numel) }
                .map_err(|_| FlameError::CudaDriver)?;
            
            let scale = 1.0 / (1.0 - self.config.dropout_p);
            let cfg = cudarc::driver::LaunchConfig::for_num_elems(numel as u32);
            
            unsafe {
                f.launch(cfg, (
                    &mut output_data,
                    &*attn_weights.data(),
                    &random_vals,
                    self.config.dropout_p,
                    scale,
                    numel as i32,
                )).map_err(|_| FlameError::Cuda("Failed to launch dropout kernel".into()))?;
            }
            
            crate::cuda_kernels::create_output_tensor(
                output_data,
                attn_weights.shape().clone(),
                self.device.clone()
            )
        } else {
            attn_weights
        };
        
        // Compute output: attn_weights @ V
        // [batch, num_heads, seq_len_q, seq_len_kv] @ [batch, num_heads, seq_len_kv, head_dim]
        // = [batch, num_heads, seq_len_q, head_dim]
        let output = attn_weights.bmm(&v)?;
        
        // Transpose back: [batch, seq_len_q, num_heads, head_dim]
        output.permute(&[0, 2, 1, 3])
    }
    
    /// Apply causal mask to attention scores
    fn apply_causal_mask(&self, scores: &Tensor) -> Result<Tensor> {
        let dims = scores.shape().dims();
        let seq_len_q = dims[2];
        let seq_len_kv = dims[3];
        
        // Create causal mask on CPU
        let mut mask_data = vec![-1e9f32; seq_len_q * seq_len_kv];
        for i in 0..seq_len_q {
            for j in 0..=i.min(seq_len_kv - 1) {
                mask_data[i * seq_len_kv + j] = 0.0;
            }
        }
        
        // Create mask tensor
        let mask = Tensor::from_vec(
            mask_data,
            Shape::from_dims(&[1, 1, seq_len_q, seq_len_kv]),
            self.device.clone()
        )?;
        
        // Add mask to scores (broadcasting will handle batch and heads dimensions)
        scores.add(&mask)
    }
    
    /// Softmax along specified dimension
    fn softmax(&self, x: &Tensor, dim: i32) -> Result<Tensor> {
        // For now, implement a simple softmax
        // TODO: Use optimized CUDA kernel
        
        let dims = x.shape().dims();
        let ndim = dims.len() as i32;
        let dim = if dim < 0 { ndim + dim } else { dim } as usize;
        
        // Compute max for numerical stability
        let max_vals = self.reduce_max(x, dim)?;
        
        // Subtract max
        let x_shifted = x.sub(&max_vals)?;
        
        // Exp
        let exp_x = self.exp(&x_shifted)?;
        
        // Sum
        let sum_exp = self.reduce_sum(&exp_x, dim)?;
        
        // Divide
        exp_x.div(&sum_exp)
    }
    
    /// Reduce max along dimension (keepdim=true)
    fn reduce_max(&self, x: &Tensor, dim: usize) -> Result<Tensor> {
        // Simple CPU implementation
        // TODO: Replace with CUDA kernel
        let dims = x.shape().dims();
        let mut new_dims = dims.to_vec();
        new_dims[dim] = 1;
        
        let data = x.to_vec()?;
        let mut result = vec![-f32::INFINITY; new_dims.iter().product()];
        
        // Calculate strides
        let mut strides = vec![1; dims.len()];
        for i in (0..dims.len() - 1).rev() {
            strides[i] = strides[i + 1] * dims[i + 1];
        }
        
        // Perform reduction
        for idx in 0..data.len() {
            let mut indices = vec![0; dims.len()];
            let mut rem = idx;
            for i in 0..dims.len() {
                indices[i] = rem / strides[i];
                rem %= strides[i];
            }
            
            indices[dim] = 0;
            let out_idx = indices.iter().zip(&strides).map(|(i, s)| i * s).sum::<usize>();
            result[out_idx] = result[out_idx].max(data[idx]);
        }
        
        Tensor::from_vec(result, Shape::from_dims(&new_dims), self.device.clone())
    }
    
    /// Reduce sum along dimension (keepdim=true)
    fn reduce_sum(&self, x: &Tensor, dim: usize) -> Result<Tensor> {
        // Simple CPU implementation
        // TODO: Replace with CUDA kernel
        let dims = x.shape().dims();
        let mut new_dims = dims.to_vec();
        new_dims[dim] = 1;
        
        let data = x.to_vec()?;
        let mut result = vec![0.0f32; new_dims.iter().product()];
        
        // Calculate strides
        let mut strides = vec![1; dims.len()];
        for i in (0..dims.len() - 1).rev() {
            strides[i] = strides[i + 1] * dims[i + 1];
        }
        
        // Perform reduction
        for idx in 0..data.len() {
            let mut indices = vec![0; dims.len()];
            let mut rem = idx;
            for i in 0..dims.len() {
                indices[i] = rem / strides[i];
                rem %= strides[i];
            }
            
            indices[dim] = 0;
            let out_idx = indices.iter().zip(&strides).map(|(i, s)| i * s).sum::<usize>();
            result[out_idx] += data[idx];
        }
        
        Tensor::from_vec(result, Shape::from_dims(&new_dims), self.device.clone())
    }
    
    /// Exp function
    fn exp(&self, x: &Tensor) -> Result<Tensor> {
        // Simple CPU implementation
        // TODO: Use CUDA kernel
        let data = x.to_vec()?;
        let result: Vec<f32> = data.iter().map(|v| v.exp()).collect();
        Tensor::from_vec(result, x.shape().clone(), self.device.clone())
    }
    
    /// Div with broadcasting
    fn div(&self, x: &Tensor, y: &Tensor) -> Result<Tensor> {
        // Simple implementation - assumes y is broadcastable to x
        // TODO: Proper broadcasting and CUDA kernel
        let x_data = x.to_vec()?;
        let y_data = y.to_vec()?;
        
        let x_dims = x.shape().dims();
        let y_dims = y.shape().dims();
        
        // Simple case: y has all dimensions as 1 except one
        let mut result = x_data.clone();
        
        // Calculate strides for broadcasting
        let mut x_strides = vec![1; x_dims.len()];
        for i in (0..x_dims.len() - 1).rev() {
            x_strides[i] = x_strides[i + 1] * x_dims[i + 1];
        }
        
        let mut y_strides = vec![1; y_dims.len()];
        for i in (0..y_dims.len() - 1).rev() {
            y_strides[i] = y_strides[i + 1] * y_dims[i + 1];
        }
        
        // Perform division with broadcasting
        for idx in 0..x_data.len() {
            let mut x_indices = vec![0; x_dims.len()];
            let mut rem = idx;
            for i in 0..x_dims.len() {
                x_indices[i] = rem / x_strides[i];
                rem %= x_strides[i];
            }
            
            // Map to y indices (considering broadcasting)
            let mut y_idx = 0;
            for i in 0..y_dims.len() {
                let y_i = if y_dims[i] == 1 { 0 } else { x_indices[i] };
                y_idx += y_i * y_strides[i];
            }
            
            result[idx] = x_data[idx] / y_data[y_idx];
        }
        
        Tensor::from_vec(result, x.shape().clone(), self.device.clone())
    }
}

/// Flash Attention v2 with additional optimizations
pub struct FlashAttentionV2 {
    base: FlashAttention,
}

impl FlashAttentionV2 {
    pub fn new(config: FlashAttentionConfig, device: Arc<CudaDevice>) -> Self {
        Self {
            base: FlashAttention::new(config, device),
        }
    }
    
    /// Forward pass with Flash Attention v2 optimizations
    pub fn forward(&self, q: &Tensor, k: &Tensor, v: &Tensor) -> Result<Tensor> {
        // For now, delegate to base implementation
        // TODO: Implement v2 specific optimizations
        self.base.forward(q, k, v)
    }
}

/// Simple API for Flash Attention v2 layer.
///
/// This implements scaled dot-product attention, `softmax(Q @ K^T . softmax_scale) @ V`.
/// Multi-query and grouped-query attention are supported by using tensors k and v with fewer heads
/// than q, the number of heads in k and v has to be divisible by the number of heads in q.
///
/// # Arguments
///
/// * `q` - Query tensor with shape `(batch, seq_len_q, num_heads, head_size)`.
/// * `k` - Key tensor with shape `(batch, seq_len_kv, num_heads_kv, head_size)`.
/// * `v` - Value tensor with shape `(batch, seq_len_kv, num_heads_kv, head_size)`.
/// * `softmax_scale` - Scale factor for the softmax.
/// * `causal` - Whether to apply causal masking.
///
/// The resulting tensor has dimensions `(batch, seq_len_q, num_heads, head_size)`.
pub fn flash_attn(
    q: &Tensor,
    k: &Tensor,
    v: &Tensor,
    softmax_scale: f32,
    causal: bool,
) -> Result<Tensor> {
    let device = q.device();
    let config = FlashAttentionConfig {
        is_causal: causal,
        softmax_scale: Some(softmax_scale),
        ..Default::default()
    };
    let flash_attn = FlashAttention::new(config, device);
    flash_attn.forward(q, k, v)
}

/// Flash Attention with variable-length sequences.
///
/// # Arguments
///
/// * `q` - Query tensor with shape `(total_q, num_heads, head_size)`.
/// * `k` - Key tensor with shape `(total_kv, num_heads_kv, head_size)`.
/// * `v` - Value tensor with shape `(total_kv, num_heads_kv, head_size)`.
/// * `seqlens_q` - Cumulative sequence lengths for queries.
/// * `seqlens_k` - Cumulative sequence lengths for keys/values.
/// * `max_seqlen_q` - Maximum sequence length in queries.
/// * `max_seqlen_k` - Maximum sequence length in keys/values.
/// * `softmax_scale` - Scale factor for the softmax.
/// * `causal` - Whether to apply causal masking.
///
/// The resulting tensor has dimensions `(total_q, num_heads, head_size)`.
pub fn flash_attn_varlen(
    q: &Tensor,
    k: &Tensor,
    v: &Tensor,
    seqlens_q: &Tensor,
    seqlens_k: &Tensor,
    max_seqlen_q: usize,
    max_seqlen_k: usize,
    softmax_scale: f32,
    causal: bool,
) -> Result<Tensor> {
    // For now, this falls back to the regular implementation
    // TODO: Implement variable-length support with cu_seqlens
    flash_attn(q, k, v, softmax_scale, causal)
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_flash_attention() -> Result<()> {
        let device = Arc::new(CudaDevice::new(0)?);
        
        let config = FlashAttentionConfig::default();
        let flash_attn = FlashAttention::new(config, device.clone());
        
        // Test tensors
        let batch_size = 2;
        let seq_len = 8;
        let num_heads = 4;
        let head_dim = 16;
        
        let q = Tensor::randn(
            Shape::from_dims(&[batch_size, seq_len, num_heads, head_dim]),
            0.0, 0.1, device.clone()
        )?;
        
        let k = Tensor::randn(
            Shape::from_dims(&[batch_size, seq_len, num_heads, head_dim]),
            0.0, 0.1, device.clone()
        )?;
        
        let v = Tensor::randn(
            Shape::from_dims(&[batch_size, seq_len, num_heads, head_dim]),
            0.0, 0.1, device.clone()
        )?;
        
        let output = flash_attn.forward(&q, &k, &v)?;
        
        // Check output shape
        assert_eq!(output.shape().dims(), &[batch_size, seq_len, num_heads, head_dim]);
        
        println!("Flash Attention test passed!");
        Ok(())
    }
    
    #[test]
    fn test_causal_flash_attention() -> Result<()> {
        let device = Arc::new(CudaDevice::new(0)?);
        
        let config = FlashAttentionConfig {
            is_causal: true,
            ..Default::default()
        };
        let flash_attn = FlashAttention::new(config, device.clone());
        
        // Test with causal mask
        let batch_size = 1;
        let seq_len = 4;
        let num_heads = 2;
        let head_dim = 8;
        
        let q = Tensor::randn(
            Shape::from_dims(&[batch_size, seq_len, num_heads, head_dim]),
            0.0, 0.1, device.clone()
        )?;
        
        let k = Tensor::randn(
            Shape::from_dims(&[batch_size, seq_len, num_heads, head_dim]),
            0.0, 0.1, device.clone()
        )?;
        
        let v = Tensor::ones(
            Shape::from_dims(&[batch_size, seq_len, num_heads, head_dim]),
            device.clone()
        )?;
        
        let output = flash_attn.forward(&q, &k, &v)?;
        
        // Check output shape
        assert_eq!(output.shape().dims(), &[batch_size, seq_len, num_heads, head_dim]);
        
        println!("Causal Flash Attention test passed!");
        Ok(())
    }
}