//! Simple test to demonstrate FLAME's gradient modification capability
//! This is the key feature that Candle lacks!

use flame_core::{
    Tensor, Shape, Result, CudaDevice, AutogradContext,
};

fn main() -> Result<()> {
    // Initialize CUDA device
    let device = CudaDevice::new(0)?;
    println!("ðŸ”¥ FLAME Gradient Modification Test\n");
    
    // Create simple tensors
    let x = Tensor::randn(Shape::from_dims(&[2, 3]), 0.0, 1.0, device.clone())?
        .requires_grad_(true);
    let w = Tensor::randn(Shape::from_dims(&[3, 4]), 0.0, 1.0, device.clone())?
        .requires_grad_(true);
    
    println!("Input shapes:");
    println!("  x: {:?}", x.shape().dims());
    println!("  w: {:?}", w.shape().dims());
    
    // Forward pass: y = x @ w
    let y = x.matmul(&w)?;
    println!("Output shape: {:?}", y.shape().dims());
    
    // Compute loss (sum of outputs)
    let loss = y.sum()?;
    let loss_value = loss.to_vec()?[0];
    println!("\nLoss value: {:.4}", loss_value);
    
    // Backward pass - compute gradients
    let gradients = AutogradContext::backward(&loss)?;
    println!("\nâœ… Backward pass completed!");
    
    // THE KEY PART - Modify gradients before using them!
    println!("\n=== Gradient Modification Demo ===");
    
    if let Some(w_grad) = gradients.get(w.id()) {
        let original_grad = w_grad.to_vec()?;
        println!("Original gradient (first 5 values): {:?}", 
                 &original_grad[..5.min(original_grad.len())]);
        
        // MODIFY THE GRADIENT - This is what Candle CANNOT do!
        // Example 1: Gradient clipping
        let clipped_grad = w_grad.mul_scalar(0.1)?;
        let clipped_values = clipped_grad.to_vec()?;
        println!("\nClipped gradient (Ã—0.1, first 5 values): {:?}", 
                 &clipped_values[..5.min(clipped_values.len())]);
        
        // Example 2: Add noise to gradient
        let noise = Tensor::randn(w_grad.shape().clone(), 0.0, 0.01, device.clone())?;
        let noisy_grad = w_grad.add(&noise)?;
        let noisy_values = noisy_grad.to_vec()?;
        println!("\nNoisy gradient (first 5 values): {:?}", 
                 &noisy_values[..5.min(noisy_values.len())]);
        
        // Example 3: Gradient normalization
        let grad_norm = original_grad.iter()
            .map(|g| g * g)
            .sum::<f32>()
            .sqrt();
        let normalized_grad = w_grad.mul_scalar(1.0 / grad_norm)?;
        let normalized_values = normalized_grad.to_vec()?;
        println!("\nNormalized gradient (first 5 values): {:?}", 
                 &normalized_values[..5.min(normalized_values.len())]);
        
        println!("\nâœ… SUCCESS! We can modify gradients in FLAME!");
        println!("   This is IMPOSSIBLE in Candle!");
    }
    
    // Test gradient accumulation
    println!("\n=== Gradient Accumulation Demo ===");
    
    // Clear previous computation graph
    AutogradContext::clear();
    
    // Simulate 3 mini-batches
    let mut accumulated_grad = None;
    for i in 0..3 {
        let x_batch = Tensor::randn(Shape::from_dims(&[2, 3]), 0.0, 1.0, device.clone())?
            .requires_grad_(true);
        
        let y_batch = x_batch.matmul(&w)?;
        let loss_batch = y_batch.sum()?;
        
        let batch_gradients = AutogradContext::backward(&loss_batch)?;
        
        if let Some(w_grad) = batch_gradients.get(w.id()) {
            match &mut accumulated_grad {
                None => accumulated_grad = Some(Clone::clone(w_grad)),
                Some(acc) => {
                    let sum = acc.add(w_grad)?;
                    accumulated_grad = Some(sum);
                }
            }
        }
        
        AutogradContext::clear();
        println!("  Batch {} gradient accumulated", i + 1);
    }
    
    // Average the accumulated gradients
    if let Some(acc) = accumulated_grad {
        let averaged = acc.mul_scalar(1.0 / 3.0)?;
        let avg_values = averaged.to_vec()?;
        println!("\nAveraged gradient (first 5 values): {:?}", 
                 &avg_values[..5.min(avg_values.len())]);
        println!("âœ… Gradient accumulation and averaging successful!");
    }
    
    println!("\n=== Summary ===");
    println!("FLAME successfully demonstrated:");
    println!("  1. âœ… Automatic gradient computation");
    println!("  2. âœ… Gradient clipping");
    println!("  3. âœ… Gradient noise injection");
    println!("  4. âœ… Gradient normalization");
    println!("  5. âœ… Gradient accumulation");
    println!("\nThese are ESSENTIAL for modern deep learning!");
    println!("Candle cannot do this - that's why FLAME exists! ðŸ”¥");
    
    Ok(())
}