#![cfg(all(feature = "legacy_examples", feature = "cudnn"))]
#![allow(unused_imports, unused_variables, unused_mut, dead_code)]
#![cfg_attr(
    clippy,
    allow(
        clippy::unused_imports,
        clippy::useless_vec,
        clippy::needless_borrow,
        clippy::needless_clone
    )
)]

use flame_core::layer_norm::LayerNorm;
use flame_core::linear::Linear;
use flame_core::{should_use_cudnn, DType, Device, Shape, Tensor};
use std::sync::Arc;

fn main() -> flame_core::Result<()> {
    println!("\n=== Testing cuDNN is Always Active ===");

    // Check global cuDNN flag
    println!("Global cuDNN enabled: {}", should_use_cudnn());

    // Create device
    let device = Device::cuda(0)?;
    let cuda_device = match &device {
        Device::Cuda(d) => d.clone(),
        _ => panic!("Expected CUDA device"),
    };

    // Test 1: Linear layer
    println!("\n--- Testing Linear Layer ---");
    let linear = Linear::new(512, 1024, true, &cuda_device)?;
    let input = Tensor::randn(Shape::from_dims(&[32, 512]), 0.0, 1.0, cuda_device.clone())?;

    println!("Running Linear forward pass...");
    let output = linear.forward(&input)?;
    println!("Linear output shape: {:?}", output.shape());

    // Test 2: LayerNorm
    println!("\n--- Testing LayerNorm ---");
    let layer_norm = LayerNorm::new(vec![512], 1e-5, cuda_device.clone())?;

    println!("Running LayerNorm forward pass...");
    let norm_output = layer_norm.forward(&input)?;
    println!("LayerNorm output shape: {:?}", norm_output.shape());

    // Test 3: Matrix multiplication
    println!("\n--- Testing MatMul ---");
    let a = Tensor::randn(Shape::from_dims(&[128, 256]), 0.0, 1.0, cuda_device.clone())?;
    let b = Tensor::randn(Shape::from_dims(&[256, 512]), 0.0, 1.0, cuda_device.clone())?;

    println!("Running MatMul...");
    let c = a.matmul(&b)?;
    println!("MatMul output shape: {:?}", c.shape());

    // Test 4: Conv2D (if available)
    #[cfg(feature = "cudnn")]
    {
        use flame_core::conv::Conv2d;

        println!("\n--- Testing Conv2D ---");
        let conv = Conv2d::new(3, 64, 3, 1, 1, 1, true, cuda_device.clone())?;
        let conv_input = Tensor::randn(
            Shape::from_dims(&[8, 3, 224, 224]),
            0.0,
            1.0,
            cuda_device.clone(),
        )?;

        println!("Running Conv2D forward pass...");
        let conv_output = conv.forward(&conv_input)?;
        println!("Conv2D output shape: {:?}", conv_output.shape());
    }

    println!("\nâœ… All operations should have used cuDNN acceleration!");
    println!("ðŸš€ 60% memory reduction active for all operations!");

    Ok(())
}
