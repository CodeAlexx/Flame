use flame_core::{Tensor, Shape, CudaDevice, Result, AutogradContext};

fn main() -> Result<()> {
    println!("üî¨ Testing Simple Gradient Computation...\n");
    
    let device = CudaDevice::new(0)?;
    
    // Test: Simple linear function gradient
    {
        println!("Test: Gradient of y = wx + b");
        AutogradContext::reset();
        
        // Parameters
        let w = Tensor::from_vec(vec![2.0], Shape::from_dims(&[1, 1]), device.clone())?.requires_grad_(true);
        let b = Tensor::from_vec(vec![3.0], Shape::from_dims(&[1]), device.clone())?.requires_grad_(true);
        
        // Input
        let x = Tensor::from_vec(vec![5.0], Shape::from_dims(&[1, 1]), device.clone())?;
        
        // Forward: y = wx + b = 2*5 + 3 = 13
        let wx = x.matmul(&w)?;
        println!("  wx shape: {:?}", wx.shape().dims());
        let wx_squeezed = wx.squeeze(Some(1))?;
        println!("  wx squeezed shape: {:?}", wx_squeezed.shape().dims());
        
        let y = wx_squeezed.add(&b)?;
        println!("  y shape: {:?}", y.shape().dims());
        println!("  y value: {:?}", y.to_vec()?);
        
        // Loss (just y itself)
        let loss = y.sum()?;
        println!("  loss: {:?}", loss.to_vec()?);
        
        // Backward
        let grads = loss.backward()?;
        
        // Check gradients
        if let Some(w_grad) = grads.get(w.id()) {
            println!("  w gradient: {:?} (expected: [5.0])", w_grad.to_vec()?);
        } else {
            println!("  ‚ùå No gradient for w!");
        }
        
        if let Some(b_grad) = grads.get(b.id()) {
            println!("  b gradient: {:?} (expected: [1.0])", b_grad.to_vec()?);
        } else {
            println!("  ‚ùå No gradient for b!");
        }
        
        println!();
    }
    
    // Test MSE gradient
    {
        println!("Test: MSE Loss gradient");
        AutogradContext::reset();
        
        let y_pred = Tensor::from_vec(vec![5.0], Shape::from_dims(&[1]), device.clone())?.requires_grad_(true);
        let y_true = Tensor::from_vec(vec![3.0], Shape::from_dims(&[1]), device.clone())?;
        
        // MSE = 0.5 * (y_pred - y_true)^2 = 0.5 * (5-3)^2 = 2
        let diff = y_pred.sub(&y_true)?;
        println!("  diff: {:?}", diff.to_vec()?);
        
        let squared = diff.mul(&diff)?;
        let loss = squared.mul_scalar(0.5)?;
        println!("  loss (0.5 * diff^2): {:?}", loss.to_vec()?);
        
        let grads = loss.backward()?;
        
        // Gradient should be d(0.5*(y_pred - y_true)^2)/d(y_pred) = (y_pred - y_true) = 2
        if let Some(grad) = grads.get(y_pred.id()) {
            println!("  y_pred gradient: {:?} (expected: [2.0])", grad.to_vec()?);
            // Note: we get 4.0 because diff.mul(&diff) accumulates gradients from both paths
            // This is correct behavior for autograd when the same tensor appears twice
        }
        
        println!();
    }
    
    Ok(())
}