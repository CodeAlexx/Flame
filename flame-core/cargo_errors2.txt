   Compiling flame-core v0.1.0 (/home/alex/diffusers-rs/flame/flame-core)
warning: flame-core@0.1.0: CUDA_HOME not set, CUDA kernels will not be available
warning: unused import: `LaunchAsync`
 --> flame-core/src/tensor_storage.rs:3:45
  |
3 | use cudarc::driver::{CudaDevice, CudaSlice, LaunchAsync};
  |                                             ^^^^^^^^^^^
  |
  = note: `#[warn(unused_imports)]` on by default

warning: unused imports: `bf16` and `f16`
 --> flame-core/src/tensor_storage.rs:4:12
  |
4 | use half::{f16, bf16};
  |            ^^^  ^^^^

warning: unused imports: `CudaFunction` and `DevicePtr`
 --> flame-core/src/tensor.rs:5:45
  |
5 | use cudarc::driver::{CudaDevice, CudaSlice, DevicePtr, LaunchConfig, LaunchAsync, DeviceSlice, CudaFunction};
  |                                             ^^^^^^^^^                                          ^^^^^^^^^^^^

warning: unused imports: `CudaSlice` and `LaunchConfig`
 --> flame-core/src/layer_norm.rs:7:34
  |
7 | use cudarc::driver::{CudaDevice, CudaSlice, LaunchConfig, LaunchAsync};
  |                                  ^^^^^^^^^  ^^^^^^^^^^^^

warning: unused imports: `CudaFunction`, `DevicePtr`, and `cublas::CudaBlas`
 --> flame-core/src/cuda_kernels_gpu.rs:4:64
  |
4 |     driver::{CudaDevice, LaunchAsync, LaunchConfig, CudaSlice, DevicePtr, CudaFunction}, 
  |                                                                ^^^^^^^^^  ^^^^^^^^^^^^
5 |     nvrtc::{compile_ptx_with_opts, CompileOptions},
6 |     cublas::CudaBlas
  |     ^^^^^^^^^^^^^^^^

warning: unused import: `std::os::raw::c_void`
  --> flame-core/src/cuda_kernels_gpu.rs:11:5
   |
11 | use std::os::raw::c_void;
   |     ^^^^^^^^^^^^^^^^^^^^

warning: unused import: `DType`
 --> flame-core/src/cuda_kernels_v2.rs:6:48
  |
6 | use crate::{Tensor, Shape, Result, FlameError, DType};
  |                                                ^^^^^

warning: unused import: `TensorId`
 --> flame-core/src/cuda_kernels_v2.rs:7:21
  |
7 | use crate::tensor::{TensorId};
  |                     ^^^^^^^^

warning: unused import: `Shape`
 --> flame-core/src/cuda_gradient_ops.rs:4:41
  |
4 | use crate::{Tensor, Result, FlameError, Shape, cuda_kernel_compiler::compile_cuda_kernel};
  |                                         ^^^^^

warning: unused import: `std::sync::Arc`
 --> flame-core/src/autograd_ops_complete.rs:6:5
  |
6 | use std::sync::Arc;
  |     ^^^^^^^^^^^^^^

warning: unused import: `CudaDevice`
 --> flame-core/src/autograd_ops_complete.rs:7:22
  |
7 | use cudarc::driver::{CudaDevice, LaunchAsync};
  |                      ^^^^^^^^^^

warning: unused import: `DType`
 --> flame-core/src/autograd.rs:6:48
  |
6 | use crate::{Tensor, Result, FlameError, Shape, DType};
  |                                                ^^^^^

warning: unused doc comment
   --> flame-core/src/autograd_v3.rs:415:1
    |
415 | /// Thread-local autograd engine
    | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ rustdoc does not generate documentation for macro invocations
    |
    = help: to document an item produced by a macro, the macro must produce the documentation as part of its expansion
    = note: `#[warn(unused_doc_comments)]` on by default

warning: unused imports: `FlameError` and `Shape`
 --> flame-core/src/optimizers.rs:1:29
  |
1 | use crate::{Tensor, Result, FlameError, Shape};
  |                             ^^^^^^^^^^  ^^^^^

warning: unused import: `std::sync::Arc`
 --> flame-core/src/optimizers.rs:2:5
  |
2 | use std::sync::Arc;
  |     ^^^^^^^^^^^^^^

warning: unused import: `cudarc::driver::CudaDevice`
 --> flame-core/src/optimizers.rs:3:5
  |
3 | use cudarc::driver::CudaDevice;
  |     ^^^^^^^^^^^^^^^^^^^^^^^^^^

warning: unused import: `FlameError`
 --> flame-core/src/pooling_impl.rs:3:36
  |
3 | use crate::{Tensor, Shape, Result, FlameError};
  |                                    ^^^^^^^^^^

warning: unused import: `LaunchConfig`
 --> flame-core/src/conv3d_simple.rs:9:35
  |
9 | use cudarc::driver::{LaunchAsync, LaunchConfig, CudaSlice};
  |                                   ^^^^^^^^^^^^

warning: unused import: `CudaDevice`
 --> flame-core/src/fused_kernels.rs:6:48
  |
6 | use crate::{Tensor, Shape, Result, FlameError, CudaDevice};
  |                                                ^^^^^^^^^^

warning: unused import: `std::sync::Arc`
 --> flame-core/src/fused_kernels.rs:8:5
  |
8 | use std::sync::Arc;
  |     ^^^^^^^^^^^^^^

warning: unused import: `CudaFunction`
 --> flame-core/src/fused_kernels.rs:9:35
  |
9 | use cudarc::driver::{LaunchAsync, CudaFunction};
  |                                   ^^^^^^^^^^^^

warning: unused import: `super::*`
   --> flame-core/src/fp16.rs:306:9
    |
306 |     use super::*;
    |         ^^^^^^^^

warning: unused imports: `CudaFunction`, `DeviceRepr`, `LaunchAsync`, and `LaunchConfig`
 --> flame-core/src/kernel_launcher.rs:8:26
  |
8 |     driver::{CudaDevice, CudaFunction, LaunchAsync, LaunchConfig, DeviceRepr},
  |                          ^^^^^^^^^^^^  ^^^^^^^^^^^  ^^^^^^^^^^^^  ^^^^^^^^^^

warning: unused import: `std::sync::Arc`
 --> flame-core/src/tensor_ops_missing.rs:4:5
  |
4 | use std::sync::Arc;
  |     ^^^^^^^^^^^^^^

warning: unused import: `FlameError`
 --> flame-core/src/parameter.rs:3:39
  |
3 | use crate::{Tensor, TensorId, Result, FlameError, Shape};
  |                                       ^^^^^^^^^^

warning: unused import: `FlameError`
 --> flame-core/src/adam.rs:3:39
  |
3 | use crate::{Tensor, TensorId, Result, FlameError, parameter::Parameter};
  |                                       ^^^^^^^^^^

warning: unused import: `FlameError`
 --> flame-core/src/lora.rs:3:36
  |
3 | use crate::{Tensor, Shape, Result, FlameError};
  |                                    ^^^^^^^^^^

warning: unused imports: `Shape` and `TensorId`
 --> flame-core/src/loss.rs:6:62
  |
6 | use crate::{Tensor, Result, FlameError, AutogradContext, Op, TensorId, Shape};
  |                                                              ^^^^^^^^  ^^^^^

warning: unused doc comment
  --> flame-core/src/gradient_checkpointing.rs:15:1
   |
15 | /// Global checkpoint manager
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ rustdoc does not generate documentation for macro invocations
   |
   = help: to document an item produced by a macro, the macro must produce the documentation as part of its expansion

warning: unused imports: `AutogradContext` and `Op`
 --> flame-core/src/gradient_checkpointing.rs:8:16
  |
8 |     autograd::{AutogradContext, Op},
  |                ^^^^^^^^^^^^^^^  ^^

warning: unused imports: `CudaStream`, `DevicePtr`, `LaunchAsync`, and `LaunchConfig`
  --> flame-core/src/gradient_checkpointing.rs:12:34
   |
12 | use cudarc::driver::{CudaDevice, CudaStream, DevicePtr, LaunchAsync, LaunchConfig};
   |                                  ^^^^^^^^^^  ^^^^^^^^^  ^^^^^^^^^^^  ^^^^^^^^^^^^

warning: unused import: `cudarc::nvrtc::compile_ptx`
  --> flame-core/src/gradient_checkpointing.rs:13:5
   |
13 | use cudarc::nvrtc::compile_ptx;
   |     ^^^^^^^^^^^^^^^^^^^^^^^^^^

warning: unused import: `TensorId`
 --> flame-core/src/sage_attention.rs:4:21
  |
4 | use crate::{Tensor, TensorId, Shape, Result, FlameError, DType};
  |                     ^^^^^^^^

warning: unused import: `CudaDevice`
 --> flame-core/src/sage_attention.rs:7:35
  |
7 | use cudarc::driver::{LaunchAsync, CudaDevice};
  |                                   ^^^^^^^^^^

warning: unused import: `std::sync::Arc`
 --> flame-core/src/sage_attention.rs:8:5
  |
8 | use std::sync::Arc;
  |     ^^^^^^^^^^^^^^

warning: unused import: `LaunchAsync`
 --> flame-core/src/layer_norm.rs:7:59
  |
7 | use cudarc::driver::{CudaDevice, CudaSlice, LaunchConfig, LaunchAsync};
  |                                                           ^^^^^^^^^^^

warning: unused variable: `blas`
   --> flame-core/src/tensor.rs:496:13
    |
496 |         let blas = CudaBlas::new(self.device.clone())
    |             ^^^^ help: if this is intentional, prefix it with an underscore: `_blas`
    |
    = note: `#[warn(unused_variables)]` on by default

warning: variable does not need to be mutable
   --> flame-core/src/tensor.rs:690:13
    |
690 |         let mut output = self.add(&neg_other)?;
    |             ----^^^^^^
    |             |
    |             help: remove this `mut`
    |
    = note: `#[warn(unused_mut)]` on by default

warning: variable does not need to be mutable
    --> flame-core/src/tensor.rs:1256:13
     |
1256 |         let mut output = Tensor {
     |             ----^^^^^^
     |             |
     |             help: remove this `mut`

warning: unused variable: `e`
   --> flame-core/src/memory_pool.rs:101:23
    |
101 |             .map_err(|e| FlameError::CudaDriver)?;
    |                       ^ help: if this is intentional, prefix it with an underscore: `_e`

warning: unused variable: `ptr`
   --> flame-core/src/memory_pool.rs:121:34
    |
121 |     pub fn deallocate(&mut self, ptr: CudaSlice<f32>) {
    |                                  ^^^ help: if this is intentional, prefix it with an underscore: `_ptr`

warning: unused variable: `bias_id`
   --> flame-core/src/conv.rs:195:17
    |
195 |             let bias_id = if let Some(bias) = &self.bias {
    |                 ^^^^^^^ help: if this is intentional, prefix it with an underscore: `_bias_id`

warning: unused variable: `bias_id`
   --> flame-core/src/cuda_conv2d.rs:206:17
    |
206 |             let bias_id = if let Some(b) = bias {
    |                 ^^^^^^^ help: if this is intentional, prefix it with an underscore: `_bias_id`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_conv2d.rs:237:13
    |
237 |         let mut result = output.clone()?;
    |             ----^^^^^^
    |             |
    |             help: remove this `mut`

warning: unused variable: `col_size`
   --> flame-core/src/cuda_conv2d.rs:289:13
    |
289 |         let col_size = batch_size * out_channels * out_height * out_width;
    |             ^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_col_size`

warning: unnecessary `unsafe` block
   --> flame-core/src/layer_norm.rs:246:5
    |
246 |     unsafe {
    |     ^^^^^^ unnecessary `unsafe` block
    |
    = note: `#[warn(unused_unsafe)]` on by default

warning: unused variable: `output`
   --> flame-core/src/flash_attention.rs:761:5
    |
761 |     output: &Tensor,
    |     ^^^^^^ help: if this is intentional, prefix it with an underscore: `_output`

warning: variable does not need to be mutable
  --> flame-core/src/cuda_kernels.rs:88:13
   |
88 |         let mut output = Tensor::zeros(a.shape.clone(), a.device.clone())?;
   |             ----^^^^^^
   |             |
   |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels.rs:119:13
    |
119 |         let mut output = Tensor::zeros(a.shape.clone(), a.device.clone())?;
    |             ----^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels.rs:143:13
    |
143 |         let mut output = Tensor::zeros(tensor.shape.clone(), tensor.device.clone())?;
    |             ----^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels.rs:167:13
    |
167 |         let mut output = Tensor::zeros(tensor.shape.clone(), tensor.device.clone())?;
    |             ----^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels.rs:191:13
    |
191 |         let mut output = Tensor::zeros(tensor.shape.clone(), tensor.device.clone())?;
    |             ----^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels.rs:215:13
    |
215 |         let mut output = Tensor::zeros(tensor.shape.clone(), tensor.device.clone())?;
    |             ----^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels.rs:239:13
    |
239 |         let mut output = Tensor::zeros(tensor.shape.clone(), tensor.device.clone())?;
    |             ----^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels.rs:263:13
    |
263 |         let mut output = Tensor::zeros(tensor.shape.clone(), tensor.device.clone())?;
    |             ----^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels.rs:287:13
    |
287 |         let mut output = Tensor::zeros(tensor.shape.clone(), tensor.device.clone())?;
    |             ----^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable `count` is assigned to, but never used
   --> flame-core/src/cuda_kernels.rs:389:21
    |
389 |             let mut count = 0;
    |                     ^^^^^
    |
    = note: consider using `_count` instead

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels.rs:366:13
    |
366 |         let mut output = Tensor::zeros(Shape::from_dims(&output_shape), tensor.device.clone())?;
    |             ----^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels.rs:443:13
    |
443 |         let mut output = Tensor::zeros(Shape::from_dims(&[cols, rows]), tensor.device.clone())?;
    |             ----^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels.rs:498:13
    |
498 |         let mut output = Tensor::zeros(tensor.shape.clone(), tensor.device.clone())?;
    |             ----^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels.rs:521:13
    |
521 |         let mut output = Tensor::zeros(tensor.shape.clone(), tensor.device.clone())?;
    |             ----^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels.rs:549:13
    |
549 |         let mut output = Tensor::zeros(tensor.shape.clone(), tensor.device.clone())?;
    |             ----^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels.rs:572:13
    |
572 |         let mut output = Tensor::zeros(tensor.shape.clone(), tensor.device.clone())?;
    |             ----^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels.rs:595:13
    |
595 |         let mut output = Tensor::zeros(tensor.shape.clone(), tensor.device.clone())?;
    |             ----^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels.rs:618:13
    |
618 |         let mut output = Tensor::zeros(tensor.shape.clone(), tensor.device.clone())?;
    |             ----^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels.rs:976:13
    |
976 |         let mut input_grad = Tensor::zeros(Shape::from_dims(input_shape), device.clone())?;
    |             ----^^^^^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
    --> flame-core/src/cuda_kernels.rs:1010:13
     |
1010 |         let mut input_grad = Tensor::zeros(Shape::from_dims(input_shape), device.clone())?;
     |             ----^^^^^^^^^^
     |             |
     |             help: remove this `mut`

warning: unused variable: `ptx`
    --> flame-core/src/cuda_kernels.rs:1048:9
     |
1048 |     let ptx = compile_ptx(kernel_code)
     |         ^^^ help: if this is intentional, prefix it with an underscore: `_ptx`

warning: unused variable: `device`
  --> flame-core/src/cuda_kernels_gpu.rs:96:16
   |
96 |     pub fn new(device: Arc<CudaDevice>) -> Result<Self> {
   |                ^^^^^^ help: if this is intentional, prefix it with an underscore: `_device`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels_gpu.rs:161:13
    |
161 |         let mut output_data = unsafe { tensor.device.alloc::<f32>(out_elements) }
    |             ----^^^^^^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels_gpu.rs:287:13
    |
287 |         let mut output_data = crate::tensor::alloc_from_pool(&a.device, numel)
    |             ----^^^^^^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels_gpu.rs:322:13
    |
322 |         let mut output_data = crate::tensor::alloc_from_pool(&a.device, numel)
    |             ----^^^^^^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels_gpu.rs:350:13
    |
350 |         let mut output_data = unsafe { tensor.device.alloc::<f32>(numel) }
    |             ----^^^^^^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels_gpu.rs:378:13
    |
378 |         let mut output_data = unsafe { tensor.device.alloc::<f32>(numel) }
    |             ----^^^^^^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels_gpu.rs:407:13
    |
407 |         let mut output_data = unsafe { tensor.device.alloc::<f32>(numel) }
    |             ----^^^^^^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels_gpu.rs:440:13
    |
440 |         let mut output_data = unsafe { tensor.device.alloc::<f32>(numel) }
    |             ----^^^^^^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels_gpu.rs:469:13
    |
469 |         let mut output_data = unsafe { tensor.device.alloc::<f32>(numel) }
    |             ----^^^^^^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels_gpu.rs:497:13
    |
497 |         let mut output_data = unsafe { tensor.device.alloc::<f32>(numel) }
    |             ----^^^^^^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels_gpu.rs:525:13
    |
525 |         let mut output_data = unsafe { tensor.device.alloc::<f32>(numel) }
    |             ----^^^^^^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels_gpu.rs:634:13
    |
634 |         let mut output_data = unsafe { tensor.device.alloc::<f32>(rows * cols) }
    |             ----^^^^^^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels_gpu.rs:670:13
    |
670 |         let mut output_data = unsafe { tensor.device.alloc::<f32>(numel) }
    |             ----^^^^^^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels_gpu.rs:696:13
    |
696 |         let mut output_data = unsafe { tensor.device.alloc::<f32>(numel) }
    |             ----^^^^^^^^^^^
    |             |
    |             help: remove this `mut`

warning: unnecessary `unsafe` block
   --> flame-core/src/cuda_kernels_gpu.rs:739:9
    |
739 |         unsafe {
    |         ^^^^^^ unnecessary `unsafe` block

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels_gpu.rs:735:13
    |
735 |         let mut output_data = unsafe { tensor.device.alloc::<f32>(numel) }
    |             ----^^^^^^^^^^^
    |             |
    |             help: remove this `mut`

warning: unnecessary `unsafe` block
   --> flame-core/src/cuda_kernels_gpu.rs:838:9
    |
838 |         unsafe {
    |         ^^^^^^ unnecessary `unsafe` block

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels_gpu.rs:832:13
    |
832 |         let mut output_data = crate::tensor::alloc_from_pool(&input.device, output_numel)
    |             ----^^^^^^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels_gpu.rs:834:13
    |
834 |         let mut indices_data = crate::tensor::alloc_from_pool(&input.device, output_numel)
    |             ----^^^^^^^^^^^^
    |             |
    |             help: remove this `mut`

warning: unnecessary `unsafe` block
    --> flame-core/src/cuda_kernels_gpu.rs:1013:9
     |
1013 |         unsafe {
     |         ^^^^^^ unnecessary `unsafe` block

warning: variable does not need to be mutable
    --> flame-core/src/cuda_kernels_gpu.rs:1009:13
     |
1009 |         let mut output_data = crate::tensor::alloc_from_pool(&input.device, output_numel)
     |             ----^^^^^^^^^^^
     |             |
     |             help: remove this `mut`

warning: unnecessary `unsafe` block
    --> flame-core/src/cuda_kernels_gpu.rs:1143:9
     |
1143 |         unsafe {
     |         ^^^^^^ unnecessary `unsafe` block

warning: variable does not need to be mutable
    --> flame-core/src/cuda_kernels_gpu.rs:1139:13
     |
1139 |         let mut grad_input = crate::tensor::alloc_from_pool(&input.device, input_numel)
     |             ----^^^^^^^^^^
     |             |
     |             help: remove this `mut`

warning: unnecessary `unsafe` block
    --> flame-core/src/cuda_kernels_gpu.rs:1228:9
     |
1228 |         unsafe {
     |         ^^^^^^ unnecessary `unsafe` block

warning: variable does not need to be mutable
    --> flame-core/src/cuda_kernels_gpu.rs:1224:13
     |
1224 |         let mut output_data = crate::tensor::alloc_from_pool(&input.device, output_numel)
     |             ----^^^^^^^^^^^
     |             |
     |             help: remove this `mut`

warning: unnecessary `unsafe` block
    --> flame-core/src/cuda_kernels_gpu.rs:1302:9
     |
1302 |         unsafe {
     |         ^^^^^^ unnecessary `unsafe` block

warning: variable does not need to be mutable
    --> flame-core/src/cuda_kernels_gpu.rs:1298:13
     |
1298 |         let mut output_data = crate::tensor::alloc_from_pool(&input.device, output_numel)
     |             ----^^^^^^^^^^^
     |             |
     |             help: remove this `mut`

warning: unnecessary `unsafe` block
    --> flame-core/src/cuda_kernels_gpu.rs:1367:9
     |
1367 |         unsafe {
     |         ^^^^^^ unnecessary `unsafe` block

warning: variable does not need to be mutable
    --> flame-core/src/cuda_kernels_gpu.rs:1363:13
     |
1363 |         let mut output_data = crate::tensor::alloc_from_pool(&input.device, output_numel)
     |             ----^^^^^^^^^^^
     |             |
     |             help: remove this `mut`

warning: unnecessary `unsafe` block
    --> flame-core/src/cuda_kernels_gpu.rs:1461:9
     |
1461 |         unsafe {
     |         ^^^^^^ unnecessary `unsafe` block

warning: variable does not need to be mutable
    --> flame-core/src/cuda_kernels_gpu.rs:1457:13
     |
1457 |         let mut output_data = crate::tensor::alloc_from_pool(&input.device, output_numel)
     |             ----^^^^^^^^^^^
     |             |
     |             help: remove this `mut`

warning: unused variable: `input_offset`
    --> flame-core/src/cuda_kernels_gpu.rs:1540:17
     |
1540 |             let input_offset = b * in_channels * h_in * w_in;
     |                 ^^^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_input_offset`

warning: unused variable: `grid_dim`
    --> flame-core/src/cuda_kernels_gpu.rs:1549:17
     |
1549 |             let grid_dim = (
     |                 ^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_grid_dim`

warning: unused variable: `block_dim`
    --> flame-core/src/cuda_kernels_gpu.rs:1554:17
     |
1554 |             let block_dim = (16, 16, 4);
     |                 ^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_block_dim`

warning: variable does not need to be mutable
    --> flame-core/src/cuda_kernels_gpu.rs:1647:13
     |
1647 |         let mut output_data = crate::tensor::alloc_from_pool(&input.device, numel)
     |             ----^^^^^^^^^^^
     |             |
     |             help: remove this `mut`

warning: unused variable: `dim`
    --> flame-core/src/cuda_kernels_gpu.rs:1667:9
     |
1667 |         dim: usize,
     |         ^^^ help: if this is intentional, prefix it with an underscore: `_dim`

warning: unnecessary `unsafe` block
    --> flame-core/src/cuda_kernels_gpu.rs:1711:9
     |
1711 |         unsafe {
     |         ^^^^^^ unnecessary `unsafe` block

warning: variable does not need to be mutable
    --> flame-core/src/cuda_kernels_gpu.rs:1696:13
     |
1696 |         let mut output_data = unsafe {
     |             ----^^^^^^^^^^^
     |             |
     |             help: remove this `mut`

warning: variable does not need to be mutable
  --> flame-core/src/cuda_kernels_v2.rs:57:13
   |
57 |         let mut output = crate::tensor::alloc_from_pool(&a.device, numel)
   |             ----^^^^^^
   |             |
   |             help: remove this `mut`

warning: variable does not need to be mutable
  --> flame-core/src/cuda_kernels_v2.rs:88:13
   |
88 |         let mut output = crate::tensor::alloc_from_pool(&a.device, numel)
   |             ----^^^^^^
   |             |
   |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels_v2.rs:111:13
    |
111 |         let mut output = crate::tensor::alloc_from_pool(&input.device, numel)
    |             ----^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels_v2.rs:133:13
    |
133 |         let mut output = crate::tensor::alloc_from_pool(&input.device, numel)
    |             ----^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/cuda_kernels_v2.rs:187:13
    |
187 |         let mut output = crate::tensor::alloc_from_pool(&a.device, output_numel)
    |             ----^^^^^^
    |             |
    |             help: remove this `mut`

warning: unused variable: `n`
   --> flame-core/src/cuda_gradient_ops.rs:241:13
    |
241 |         let n = grad.shape().elem_count();
    |             ^ help: if this is intentional, prefix it with an underscore: `_n`

warning: unused variable: `total_elements`
   --> flame-core/src/cuda_tensor_gpu.rs:300:13
    |
300 |         let total_elements = self.shape.elem_count();
    |             ^^^^^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_total_elements`

warning: unused variable: `sum_buf`
   --> flame-core/src/cuda_tensor_gpu.rs:303:13
    |
303 |         let sum_buf = unsafe {
    |             ^^^^^^^ help: if this is intentional, prefix it with an underscore: `_sum_buf`

warning: unused variable: `device`
  --> flame-core/src/autograd_ops_complete.rs:28:9
   |
28 |     let device = input.device();
   |         ^^^^^^ help: if this is intentional, prefix it with an underscore: `_device`

warning: unused variable: `batch_size`
  --> flame-core/src/autograd_ops_complete.rs:37:9
   |
37 |     let batch_size = input.shape().elem_count() / norm_size;
   |         ^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_batch_size`

warning: unused variable: `mean`
  --> flame-core/src/autograd_ops_complete.rs:23:5
   |
23 |     mean: &Tensor,
   |     ^^^^ help: if this is intentional, prefix it with an underscore: `_mean`

warning: unused variable: `device`
   --> flame-core/src/autograd_ops_complete.rs:108:9
    |
108 |     let device = input.device();
    |         ^^^^^^ help: if this is intentional, prefix it with an underscore: `_device`

warning: unused variable: `channels`
   --> flame-core/src/autograd_ops_complete.rs:143:9
    |
143 |     let channels = shape[1];
    |         ^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_channels`

warning: unused variable: `i`
   --> flame-core/src/autograd_ops_complete.rs:352:9
    |
352 |     for i in 0..offset {
    |         ^ help: if this is intentional, prefix it with an underscore: `_i`

warning: unused variable: `output`
   --> flame-core/src/autograd_ops_complete.rs:660:5
    |
660 |     output: Option<&Tensor>,
    |     ^^^^^^ help: if this is intentional, prefix it with an underscore: `_output`

warning: unreachable pattern
    --> flame-core/src/autograd.rs:1196:9
     |
1196 |         _ => {
     |         ^ no value can reach this
     |
note: multiple earlier patterns match some of the same values
    --> flame-core/src/autograd.rs:1196:9
     |
344  |         Op::Add { lhs, rhs } => {
     |         -------------------- matches some of the same values
...
370  |         Op::Sub { lhs, rhs } => {
     |         -------------------- matches some of the same values
...
379  |         Op::Mul { lhs, rhs } => {
     |         -------------------- matches some of the same values
...
402  |         Op::MulScalar { input, scalar } => {
     |         ------------------------------- matches some of the same values
...
1196 |         _ => {
     |         ^ ...and 40 other patterns collectively make this unreachable
     = note: `#[warn(unreachable_patterns)]` on by default

warning: unused variable: `has_affine`
   --> flame-core/src/autograd.rs:558:17
    |
558 |             let has_affine = entry.saved_tensors.len() > 1;
    |                 ^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_has_affine`

warning: unused variable: `dim`
   --> flame-core/src/autograd.rs:803:36
    |
803 |         Op::SumDimKeepdim { input, dim } => {
    |                                    ^^^ help: try ignoring the field: `dim: _`

warning: unused variable: `sizes`
   --> flame-core/src/autograd.rs:886:28
    |
886 |         Op::Split { input, sizes, dim } => {
    |                            ^^^^^ help: try ignoring the field: `sizes: _`

warning: unused variable: `input_size`
   --> flame-core/src/autograd.rs:891:17
    |
891 |             let input_size = input_tensor.shape().dims()[*dim];
    |                 ^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_input_size`

warning: unused variable: `num_channels`
    --> flame-core/src/autograd.rs:1080:17
     |
1080 |             let num_channels = shape[1];
     |                 ^^^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_num_channels`

warning: unused variable: `bias_tensor`
    --> flame-core/src/autograd.rs:1093:17
     |
1093 |             let bias_tensor = bias.and_then(|b| entry.saved_tensors.get(&b));
     |                 ^^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_bias_tensor`

warning: unused variable: `device`
   --> flame-core/src/autograd.rs:340:5
    |
340 |     device: &Arc<CudaDevice>
    |     ^^^^^^ help: if this is intentional, prefix it with an underscore: `_device`

warning: variable does not need to be mutable
   --> flame-core/src/autograd.rs:824:17
    |
824 |             let mut weight_grad = Tensor::zeros(weight_tensor.shape().clone(), weight_tensor.device().clone())?;
    |                 ----^^^^^^^^^^^
    |                 |
    |                 help: remove this `mut`

warning: variable does not need to be mutable
    --> flame-core/src/autograd.rs:1051:17
     |
1051 |             let mut grad_log_probs = Tensor::zeros(log_probs_tensor.shape().clone(), log_probs_tensor.device().clone())?;
     |                 ----^^^^^^^^^^^^^^
     |                 |
     |                 help: remove this `mut`

warning: variable does not need to be mutable
    --> flame-core/src/autograd.rs:1149:17
     |
1149 |             let mut grads = vec![
     |                 ----^^^^^
     |                 |
     |                 help: remove this `mut`

warning: unused variable: `i`
   --> flame-core/src/autograd_v3.rs:333:21
    |
333 |                 for i in 0..grad_shape.len() - 1 {
    |                     ^ help: if this is intentional, prefix it with an underscore: `_i`

warning: variable does not need to be mutable
   --> flame-core/src/autograd_v3.rs:389:13
    |
389 |         let mut grad_in = crate::tensor::alloc_from_pool(&input.device, numel)
    |             ----^^^^^^^
    |             |
    |             help: remove this `mut`

warning: unused variable: `output_shape`
  --> flame-core/src/pooling.rs:55:13
   |
55 |         let output_shape = Shape::from_dims(&[batch, channels, h_out, w_out]);
   |             ^^^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_output_shape`

warning: unused variable: `indices`
  --> flame-core/src/pooling.rs:74:9
   |
74 |         indices: Option<&Tensor>,
   |         ^^^^^^^ help: if this is intentional, prefix it with an underscore: `_indices`

warning: unused variable: `output_shape`
   --> flame-core/src/pooling.rs:136:13
    |
136 |         let output_shape = Shape::from_dims(&[batch, channels, h_out, w_out]);
    |             ^^^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_output_shape`

warning: unused variable: `kernel_h`
   --> flame-core/src/pooling.rs:185:13
    |
185 |         let kernel_h = h_in - (h_out - 1) * stride_h;
    |             ^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_kernel_h`

warning: unused variable: `kernel_w`
   --> flame-core/src/pooling.rs:186:13
    |
186 |         let kernel_w = w_in - (w_out - 1) * stride_w;
    |             ^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_kernel_w`

warning: unused variable: `output_shape`
   --> flame-core/src/pooling.rs:188:13
    |
188 |         let output_shape = Shape::from_dims(&[batch, channels, h_out, w_out]);
    |             ^^^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_output_shape`

warning: unused variable: `kernel_h`
   --> flame-core/src/pooling.rs:250:13
    |
250 |         let kernel_h = h_in - (h_out - 1) * stride_h;
    |             ^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_kernel_h`

warning: unused variable: `kernel_w`
   --> flame-core/src/pooling.rs:251:13
    |
251 |         let kernel_w = w_in - (w_out - 1) * stride_w;
    |             ^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_kernel_w`

warning: unused variable: `output_shape`
   --> flame-core/src/pooling.rs:253:13
    |
253 |         let output_shape = Shape::from_dims(&[batch, channels, h_out, w_out]);
    |             ^^^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_output_shape`

warning: unused variable: `output_shape`
  --> flame-core/src/upsampling.rs:83:13
   |
83 |         let output_shape = Shape::from_dims(&[batch, channels, h_out, w_out]);
   |             ^^^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_output_shape`

warning: variable does not need to be mutable
   --> flame-core/src/conv3d_simple.rs:261:13
    |
261 |         let mut output_data = crate::tensor::alloc_from_pool(&self.device, output_numel)
    |             ----^^^^^^^^^^^
    |             |
    |             help: remove this `mut`

warning: unused variable: `training`
   --> flame-core/src/conv3d_simple.rs:382:47
    |
382 |     pub fn forward(&mut self, input: &Tensor, training: bool) -> Result<Tensor> {
    |                                               ^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_training`

warning: variable does not need to be mutable
   --> flame-core/src/conv3d_simple.rs:439:13
    |
439 |         let mut output_data = crate::tensor::alloc_from_pool(&self.device, numel)
    |             ----^^^^^^^^^^^
    |             |
    |             help: remove this `mut`

warning: variable does not need to be mutable
  --> flame-core/src/fused_kernels.rs:55:13
   |
55 |         let mut output = Tensor::zeros(x.shape().clone(), x.device.clone())?;
   |             ----^^^^^^
   |             |
   |             help: remove this `mut`

warning: variable does not need to be mutable
   --> flame-core/src/fused_kernels.rs:175:13
    |
175 |         let mut output = Tensor::zeros(
    |             ----^^^^^^
    |             |
    |             help: remove this `mut`

warning: unused variable: `kernel_code`
   --> flame-core/src/fused_kernels.rs:222:13
    |
222 |         let kernel_code = r#"
    |             ^^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_kernel_code`

warning: unused variable: `scale`
   --> flame-core/src/fused_kernels.rs:219:9
    |
219 |         scale: f32,
    |         ^^^^^ help: if this is intentional, prefix it with an underscore: `_scale`

warning: unused variable: `kernel_code`
   --> flame-core/src/fused_kernels.rs:343:13
    |
343 |         let kernel_code = r#"
    |             ^^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_kernel_code`

warning: variable does not need to be mutable
   --> flame-core/src/fused_kernels.rs:458:13
    |
458 |         let mut grad_input = Tensor::zeros(input.shape().clone(), input.device.clone())?;
    |             ----^^^^^^^^^^
    |             |
    |             help: remove this `mut`

warning: unused variable: `i`
   --> flame-core/src/fp16.rs:259:18
    |
259 |             for (i, (fp32_param, grad)) in self.fp32_params.iter_mut().zip(unscaled_grads.iter()).enumerate() {
    |                  ^ help: if this is intentional, prefix it with an underscore: `_i`

warning: unused variable: `kernel`
  --> flame-core/src/kernel_launcher.rs:82:9
   |
82 |         kernel: &CompiledKernel,
   |         ^^^^^^ help: if this is intentional, prefix it with an underscore: `_kernel`

warning: unused variable: `module_name`
  --> flame-core/src/kernel_launcher.rs:83:9
   |
83 |         module_name: &str,
   |         ^^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_module_name`

warning: unused variable: `i`
   --> flame-core/src/tensor_ops_extended.rs:870:18
    |
870 |             for (i, tensor) in result.iter_mut().enumerate() {
    |                  ^ help: if this is intentional, prefix it with an underscore: `_i`

warning: unused variable: `dtype`
   --> flame-core/src/gradient_checkpointing.rs:183:54
    |
183 |             CheckpointedTensor::OnCPU { data, shape, dtype } => {
    |                                                      ^^^^^ help: try ignoring the field: `dtype: _`

warning: unused variable: `input_info`
   --> flame-core/src/gradient_checkpointing.rs:240:13
    |
240 |         let input_info: Vec<(Shape, Arc<CudaDevice>)> = inputs.iter()
    |             ^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_input_info`

warning: unused variable: `causal`
   --> flame-core/src/sage_attention.rs:396:5
    |
396 |     causal: bool,
    |     ^^^^^^ help: if this is intentional, prefix it with an underscore: `_causal`

warning: unused variable: `quantized`
   --> flame-core/src/sage_attention.rs:397:5
    |
397 |     quantized: bool,
    |     ^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_quantized`

warning: function `broadcast_shapes` is never used
  --> flame-core/src/tensor.rs:33:4
   |
33 | fn broadcast_shapes(shape1: &[usize], shape2: &[usize]) -> Result<Vec<usize>> {
   |    ^^^^^^^^^^^^^^^^
   |
   = note: `#[warn(dead_code)]` on by default

warning: methods `reshape_for_bmm` and `slice_internal` are never used
   --> flame-core/src/tensor.rs:567:8
    |
88  | impl Tensor {
    | ----------- methods in this implementation
...
567 |     fn reshape_for_bmm(&self, target_batch: &[usize], m: usize, n: usize) -> Result<Tensor> {
    |        ^^^^^^^^^^^^^^^
...
611 |     fn slice_internal(&self, start: usize, len: usize) -> Result<Tensor> {
    |        ^^^^^^^^^^^^^^

warning: field `device` is never read
   --> flame-core/src/memory_pool.rs:299:5
    |
298 | pub struct Workspace {
    |            --------- field in this struct
299 |     device: Arc<CudaDevice>,
    |     ^^^^^^

warning: function `alloc_and_copy_to_pool` is never used
  --> flame-core/src/cuda_conv2d.rs:14:4
   |
14 | fn alloc_and_copy_to_pool<T: AsRef<[f32]>>(device: &Arc<CudaDevice>, data: T) -> Result<CudaSlice<f32>> {
   |    ^^^^^^^^^^^^^^^^^^^^^^

warning: function `alloc_and_copy_to_pool` is never used
  --> flame-core/src/cuda_kernels_gpu.rs:23:4
   |
23 | fn alloc_and_copy_to_pool<T: AsRef<[f32]>>(device: &Arc<CudaDevice>, data: T) -> Result<CudaSlice<f32>> {
   |    ^^^^^^^^^^^^^^^^^^^^^^

warning: constant `UPDATE_WEIGHTS_KERNEL` is never used
  --> flame-core/src/cuda_kernels_gpu.rs:49:7
   |
49 | const UPDATE_WEIGHTS_KERNEL: &str = "update_weights_kernel";
   |       ^^^^^^^^^^^^^^^^^^^^^

warning: function `get_flash_attention_backward_kernel` is never used
   --> flame-core/src/autograd_ops_complete.rs:707:4
    |
707 | fn get_flash_attention_backward_kernel() -> &'static str {
    |    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

warning: methods `extract_head` and `softmax` are never used
   --> flame-core/src/attention.rs:156:8
    |
37  | impl MultiHeadAttention {
    | ----------------------- methods in this implementation
...
156 |     fn extract_head(&self, tensor: &Tensor, batch: usize, head: usize, dim1: usize, dim2: usize) -> Result<Tensor> {
    |        ^^^^^^^^^^^^
...
193 |     fn softmax(&self, tensor: &Tensor, dim: usize) -> Result<Tensor> {
    |        ^^^^^^^

warning: function `alloc_and_copy_to_pool` is never used
  --> flame-core/src/conv3d_simple.rs:21:4
   |
21 | fn alloc_and_copy_to_pool<T: AsRef<[f32]>>(device: &Arc<CudaDevice>, data: T) -> Result<CudaSlice<f32>> {
   |    ^^^^^^^^^^^^^^^^^^^^^^

warning: fields `num_train_timesteps`, `noise_schedule`, `prediction_type`, `alphas`, and `betas` are never read
   --> flame-core/src/samplers.rs:455:5
    |
454 | pub struct DDPMScheduler {
    |            ------------- fields in this struct
455 |     num_train_timesteps: usize,
    |     ^^^^^^^^^^^^^^^^^^^
456 |     noise_schedule: String,
    |     ^^^^^^^^^^^^^^
457 |     prediction_type: String,
    |     ^^^^^^^^^^^^^^^
458 |     alphas: Vec<f32>,
    |     ^^^^^^
459 |     betas: Vec<f32>,
    |     ^^^^^

warning: field `device` is never read
  --> flame-core/src/fp16.rs:44:5
   |
41 | pub struct AutocastContext {
   |            --------------- field in this struct
...
44 |     device: Arc<CudaDevice>,
   |     ^^^^^^

warning: fields `precision` and `device` are never read
   --> flame-core/src/fp16.rs:127:5
    |
126 | pub struct MixedPrecisionTraining {
    |            ---------------------- fields in this struct
127 |     precision: Precision,
    |     ^^^^^^^^^
...
132 |     device: Arc<CudaDevice>,
    |     ^^^^^^

warning: field `base_optimizer` is never read
   --> flame-core/src/fp16.rs:203:5
    |
202 | pub struct FP16Optimizer<O> {
    |            ------------- field in this struct
203 |     base_optimizer: O,
    |     ^^^^^^^^^^^^^^

warning: field `device` is never read
  --> flame-core/src/lora.rs:39:5
   |
29 | pub struct LoRALayer {
   |            --------- field in this struct
...
39 |     device: Arc<CudaDevice>,
   |     ^^^^^^

warning: fields `shape` and `dtype` are never read
  --> flame-core/src/gradient_checkpointing.rs:52:9
   |
50 |     Deleted {
   |     ------- fields in this variant
51 |         compute_fn: Box<dyn Fn() -> Result<Tensor> + Send + Sync>,
52 |         shape: Shape,
   |         ^^^^^
53 |         dtype: DType,
   |         ^^^^^

warning: function `get_async_memcpy_kernel` is never used
   --> flame-core/src/gradient_checkpointing.rs:280:4
    |
280 | fn get_async_memcpy_kernel() -> &'static str {
    |    ^^^^^^^^^^^^^^^^^^^^^^^

warning: unused `std::result::Result` that must be used
   --> flame-core/src/lib.rs:5:18
    |
5   |           unsafe { $func.launch($cfg, ($($args,)*)) }
    |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |
   ::: flame-core/src/tensor.rs:139:9
    |
139 | /         crate::launch_kernel!(f, cfg,
140 | |             self.storage.as_slice(),
141 | |             mask.storage.as_slice(),
142 | |             &output_data,
143 | |             value,
144 | |             n as i32
145 | |         );
    | |_________- in this macro invocation
    |
    = note: this `Result` may be an `Err` variant, which should be handled
    = note: `#[warn(unused_must_use)]` on by default
    = note: this warning originates in the macro `crate::launch_kernel` (in Nightly builds, run with -Z macro-backtrace for more info)
help: use `let _ = ...` to ignore the resulting value
    |
5   |         unsafe { let _ = $func.launch($cfg, ($($args,)*)); }
    |                  +++++++                                 +

warning: `flame-core` (lib) generated 178 warnings (run `cargo fix --lib -p flame-core` to apply 93 suggestions)
warning: unused variable: `tensor_grad`
  --> flame-core/src/bin/minimal_test.rs:38:9
   |
38 |     let tensor_grad = match Tensor::zeros(Shape::from_dims(&[2, 2]), device.clone()) {
   |         ^^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_tensor_grad`
   |
   = note: `#[warn(unused_variables)]` on by default

warning: unused variable: `y`
  --> flame-core/src/bin/minimal_flame_test.rs:32:9
   |
32 |     let y = match x.add(&x) {
   |         ^ help: if this is intentional, prefix it with an underscore: `_y`
   |
   = note: `#[warn(unused_variables)]` on by default

warning: unused variable: `sum_result`
  --> flame-core/src/bin/minimal_flame_test.rs:64:9
   |
64 |     let sum_result = match z.sum() {
   |         ^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_sum_result`

warning: unused variable: `d`
  --> flame-core/src/bin/basic_ops_test.rs:21:9
   |
21 |     let d = a.add(&b)?;
   |         ^ help: if this is intentional, prefix it with an underscore: `_d`
   |
   = note: `#[warn(unused_variables)]` on by default

warning: unused variable: `e`
  --> flame-core/src/bin/basic_ops_test.rs:24:9
   |
24 |     let e = a.mul(&b)?;
   |         ^ help: if this is intentional, prefix it with an underscore: `_e`

warning: unused variable: `f`
  --> flame-core/src/bin/basic_ops_test.rs:27:9
   |
27 |     let f = a.add_scalar(5.0)?;
   |         ^ help: if this is intentional, prefix it with an underscore: `_f`

warning: unused variable: `handle`
  --> flame-core/src/bin/debug_autograd.rs:19:9
   |
19 |     let handle = std::thread::spawn(move || {
   |         ^^^^^^ help: if this is intentional, prefix it with an underscore: `_handle`
   |
   = note: `#[warn(unused_variables)]` on by default

warning: `flame-core` (bin "minimal_test") generated 1 warning
warning: `flame-core` (bin "basic_ops_test") generated 3 warnings
warning: `flame-core` (bin "minimal_flame_test") generated 2 warnings
warning: `flame-core` (bin "debug_autograd") generated 1 warning
    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.41s
